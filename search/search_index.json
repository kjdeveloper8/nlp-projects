{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NLP Roadmap","text":"<p>Natural Language Processing: Text tells the story (\u2727\u15dc\u2727)</p>"},{"location":"#toc","title":"TOC","text":"<ul> <li>Text Preprocessing<ul> <li>Normalization</li> <li>Tokenization</li> <li>Remove stop words</li> <li>Stemming</li> <li>Lemmatization</li> </ul> </li> <li>Parser<ul> <li>JSON parser</li> <li>HTML parser</li> </ul> </li> <li>Text encoding<ul> <li>BOW</li> <li>One hot encoding</li> <li>TF-IDF</li> </ul> </li> <li>Text Classification<ul> <li>Intent detection</li> <li>Named Entity Recognition</li> </ul> </li> <li>Text similarity<ul> <li>Embeddings: word2vec, TF-IDF</li> <li>Matrices: cosinse similarity, Jaccard similarity, Euclidean Distance</li> <li>Lexical similarity: for clustering and keyword matching</li> <li>Semantic simialrity: for knowledge base, string and statical based</li> <li>Algo: Global matrix factorization, Local context window</li> <li>Model: BERT</li> </ul> </li> <li>Text Clustering<ul> <li>K means</li> </ul> </li> <li>Sentiment analysis<ul> <li>Fine-grained Sentiment Analysis (like 5 star rating - scale base)</li> <li>Emotion detection (happy, sad, anger)</li> <li>Aspect-based Sentiment Analysis (customer review on new product like new headphone design)</li> <li>Intent Based Analysis (intent focused)</li> </ul> </li> <li>Languages<ul> <li>POS    </li> <li>Language detection</li> <li>Machine translation</li> </ul> </li> <li>Spell correction</li> <li>Pii</li> <li>Conversational<ul> <li>Question answer</li> <li>Text summarization</li> </ul> </li> <li>Evaluation matrices<ul> <li>F1 score</li> <li>Perplexity</li> <li>BERTScore</li> <li>BLEU (Bilingual Evaluation Understudy)</li> <li>ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</li> <li>METEOR (Metric for Evaluation of Translation with Explicit ORdering)</li> </ul> </li> <li>Transformer<ul> <li>Self Attention</li> <li>Transformer Architecture</li> </ul> </li> </ul>"},{"location":"conversational/question_answer/","title":"Question Answer in NLP","text":"<p>Question Answering (QA) in AI refers to the capability of a machine to respond to questions asked in natural language. The main objective of this technology is to extract relevant information from vast amounts of data and present it in the form of a concise answer. </p> <p>QA models take in a question and then process a large amount of text data to determine the most accurate answer. For example, if the question is \"What is the color of banana\" the QA model will scan its database and return the answer \"Yellow\". </p> <p>Nowadays, the demand for conversational AI systems and virtual assistants has grown, which has driven the development of Question Answering in NLP. These systems rely on NLP techniques like text classification, sentiment analysis, information retrieval, machine translation, and generate answers.</p>"},{"location":"conversational/question_answer/#qa-system-flow","title":"\u27a4 QA system flow","text":""},{"location":"conversational/question_answer/#1-data-collection-and-preprocessing","title":"1. Data Collection and Preprocessing","text":"<p>First is to collect a large corpus of text data from sources like books, online news articles, or databases. After that cleaning and preprocessing techniques like tokenization, steeming and lemmatization is done in order to remove irrelevant information.</p>"},{"location":"conversational/question_answer/#2-information-retrieval","title":"2. Information Retrieval","text":"<p>Information retrival can be done by algorithms that can extract relevant information from the text corpus to answer questions. This includes ner, semantic search, keyword search, text classification. </p>"},{"location":"conversational/question_answer/#3-question-analysis","title":"3. Question Analysis","text":"<p>It's important to analyze the question to understand its intent and identify keywords or phrases that will guide the information retrieval process. This can involve using techniques like POS tagging, dependency parsing, and named entity recognition to identify important words and phrases in the question.</p>"},{"location":"conversational/question_answer/#4-answer-generation","title":"4. Answer Generation","text":"<p>This often involves techniques like text generation and summarization for answer. For example, a text generation algorithm can generate a response based on the most relevant information retrieved in the previous step.</p>"},{"location":"conversational/question_answer/#5-model-training-and-evaluation","title":"5. Model Training and Evaluation","text":"<p>Model is trained to identify patterns in the data and improve the accuracy of the answers generated. For evaluation metrics like precision, recall, and F1 score is used to evaluates the performance of the QA system.</p>"},{"location":"conversational/question_answer/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>QA with Huggingface model <code>BertForQuestionAnswering</code> which is fine tuned on Stanford Question Answering Dataset (SQuAD) dataset</p>"},{"location":"conversational/question_answer/#import","title":"Import","text":"<pre><code>import torch\nfrom transformers import BertForQuestionAnswering, BertTokenizer\n</code></pre>"},{"location":"conversational/question_answer/#load-model","title":"Load model","text":"<pre><code>model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\ntokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n</code></pre>"},{"location":"conversational/question_answer/#sample-qa","title":"Sample qa","text":"<pre><code>question = \"Where is the Great Barrier Reef located?\"\nanswer_text = \"The Great Barrier Reef is located in the Coral Sea, off the coast of Australia. It is the largest coral reef system in the world, stretching over 2,300 km and covering an area of approximately 344,400 km\u00b2. The Great Barrier Reef is home to a diverse range of marine life and is considered one of the seven natural wonders of the world. It is also a UNESCO World Heritage Site threatened by climate change and other environmental factors.\"\n</code></pre>"},{"location":"conversational/question_answer/#tokenization-and-attention-masking","title":"Tokenization and attention masking","text":"<pre><code>input_ids = tokenizer.encode(question, answer_text)\nattention_mask = [1] * len(input_ids)\n</code></pre>"},{"location":"conversational/question_answer/#get-the-logits","title":"Get the logits","text":"<pre><code>output = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))\nstart_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(tokenizer.sep_token_id)])\nend_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(tokenizer.sep_token_id)])\n</code></pre>"},{"location":"conversational/question_answer/#decode-the-answer","title":"Decode the answer","text":"<pre><code>answer = tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)\n# answer: 'coral sea'\n</code></pre>"},{"location":"conversational/text_summarization/","title":"Text Summarization","text":"<p>Text summarization refers to a group of methods that employ algorithms to compress a certain amount of text while preserving the it's key points.  Systems capable of extracting the key concepts from the text while maintaining the overall meaning have the potential to revolutionize a variety of industries, including banking, law, and even healthcare.</p> <p>\u27a4 Approach </p> <ul> <li>Extractive Summarization </li> <li>Abstractive Summarization</li> </ul>"},{"location":"conversational/text_summarization/#extractive-summarization","title":"\u27a4 Extractive Summarization","text":"<p>Extractive approach is simple and uses traditional algorithms. For example, If we want to summarize our text on the basis of the frequency method, for that we will store all the unique words and frequency of all those words in the dictionary. On the basis of high frequency words, we store the sentences containing that word in our final summary. This means the words which are in our summary confirm that they are part of the given text.</p>"},{"location":"conversational/text_summarization/#abstractive-summarization","title":"\u27a4 Abstractive Summarization","text":"<p>Abstractive summarization techniques emulate human writing by generating entirely new sentences to convey key concepts from the source text, rather than rephrasing portions of it. These fresh sentences distill the vital information while eliminating irrelevant details, often incorporating novel vocabulary absent in the original text. It understands the meaning and context of the text and then generates the summary. It requires a deeper understanding of the content and the ability to generate new text without changing the meaning of the source information.</p>"},{"location":"conversational/text_summarization/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>With pytextrank</p> <pre><code>!pip install pytextrank -q\n</code></pre> <p>Import and load model</p> <pre><code>import spacy\nimport pytextrank\nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"textrank\")\n</code></pre> <p>Sample text</p> <pre><code>sample_text = \"\"\" Deep learning (also known as deep structured learning) is part of a \nbroader family of machine learning methods based on artificial neural networks with \nrepresentation learning. Learning can be supervised, semi-supervised or unsupervised. \nDeep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, \nrecurrent neural networks and convolutional neural networks have been applied to\nfields including computer vision, speech recognition, natural language processing, \nmachine translation, bioinformatics, drug design, medical image analysis, material\ninspection and board game programs, where they have produced results comparable to \nand in some cases surpassing human expert performance. Artificial neural networks\n(ANNs) were inspired by information processing and distributed communication nodes\nin biological systems. ANNs have various differences from biological brains. Specifically, \nneural networks tend to be static and symbolic, while the biological brain of most living organisms\nis dynamic (plastic) and analogue. The adjective \"deep\" in deep learning refers to the use of multiple\nlayers in the network. Early work showed that a linear perceptron cannot be a universal classifier, \nbut that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.\nDeep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, \nwhich permits practical application and optimized implementation, while retaining theoretical universality \nunder mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely \nfrom biologically informed connectionist models, for the sake of efficiency, trainability and understandability, \nwhence the structured part.\n\"\"\"\n</code></pre> <p>Summary</p> <pre><code>for text in doc._.textrank.summary(limit_phrases=2, limit_sentences=2):\n    print(text)\n    print('Summary Length:',len(text))\n</code></pre> <p>Result</p> <pre><code>recurrent neural networks and convolutional neural networks have been applied to\nfields including computer vision, speech recognition, natural language processing, \nmachine translation, bioinformatics, drug design, medical image analysis, material\ninspection and board game programs, where they have produced results comparable to \nand in some cases surpassing human expert performance.\nSummary Length: 81\nThe adjective \"deep\" in deep learning refers to the use of multiple\nlayers in the network.\nSummary Length: 20\n</code></pre>"},{"location":"languages/lang_detection/","title":"Language Detection","text":"<p>Language comes from Old French langage, based on Latin word lingua means 'tongue'. Language is a structured system of communication that consists of grammar and vocabulary but it convey more than just communication. It has a very long evolutionary history of itself. Languages are such a amazing thing! isn't it ^^</p> <p>So let's identify these languages.</p>"},{"location":"languages/lang_detection/#language-detection-with-spacy","title":"Language Detection with Spacy","text":"<pre><code>import spacy\nfrom spacy.language import Language\nfrom spacy_language_detection import LanguageDetector\n\ndef get_lang_detector(nlp, name):\n    return LanguageDetector(seed=42) \n\n\nnlp_model = spacy.load(\"en_core_web_sm\")\n# Language.factory(\"language_detector\", func=get_lang_detector)\nnlp_model.add_pipe('language_detector', last=True)\n\n# Document level language detection\njob_title = \"Senior NLP Research Engineer\"\ndoc = nlp_model(job_title)\nlanguage = doc._.language\nprint(language)\n\n# Sentence level language detection\ntext = \"\u3053\u3093\u306b\u3061\u306f \u304a\u5143\u6c17\u3067\u3059\u304b.  This is English text. \u0928\u092e\u0938\u094d\u0924\u0947, \u0906\u092a \u0915\u0948\u0938\u0947 \u0939\u0948\u0902. Er lebt mit seinen Eltern und seiner Schwester in Berlin. Yo me divierto todos los d\u00edas en el parque. Je m'appelle Ang\u00e9lica Summer, j'ai 12 ans et je suis canadienne. \u0c39\u0c32\u0c4b, \u0c0e\u0c32\u0c3e \u0c09\u0c28\u0c4d\u0c28\u0c3e\u0c30\u0c41.\"\ndoc = nlp_model(text)\nfor i, sent in enumerate(doc.sents):\n    print(sent, sent._.language)\n</code></pre> <p>Result</p> <pre><code>{'language': 'en', 'score': 0.9999944616311092}\n\u3053\u3093\u306b\u3061\u306f \u304a\u5143\u6c17\u3067\u3059\u304b.   {'language': 'ja', 'score': 0.9999999999820187}\nThis is English text. {'language': 'en', 'score': 0.9999987929307772}\n\u0928\u092e\u0938\u094d\u0924\u0947, \u0906\u092a \u0915\u0948\u0938\u0947 \u0939\u0948\u0902. {'language': 'hi', 'score': 0.9999969329463939}\nEr lebt mit seinen Eltern und seiner Schwester in Berlin. {'language': 'de', 'score': 0.999996045846908}\nYo me divierto todos los d\u00edas en el parque. {'language': 'es', 'score': 0.9999960751128255}\nJe m'appelle Ang\u00e9lica Summer, j'ai 12 ans et je suis canadienne. {'language': 'fr', 'score': 0.9999960488878061}\n\u0c39\u0c32\u0c4b, \u0c0e\u0c32\u0c3e \u0c09\u0c28\u0c4d\u0c28\u0c3e\u0c30\u0c41. {'language': 'te', 'score': 0.9999999998909419}\n</code></pre>"},{"location":"languages/lang_detection/#language-detection-with-hf","title":"Language Detection with HF","text":"<pre><code>from transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"\u3053\u3093\u306b\u3061\u306f \u304a\u5143\u6c17\u3067\u3059\u304b.\",\n    \"\u0b95\u0bbe\u0bb2\u0bc8 \u0bb5\u0ba3\u0b95\u0bcd\u0b95\u0bae\u0bcd.\",\n    \"Oh, \u00bfviste ese vestido colorido?\",\n    \"\u0398\u03b1 \u03c0\u03ac\u03c9 \u03b5\u03ba\u03b5\u03af \u03bd\u03b1 \u03b4\u03c9 \u03bb\u03bf\u03c5\u03bb\u03bf\u03cd\u03b4\u03b9\u03b1\",\n    \"J'aime manger du chocolat\",\n    \"Ich werde dorthin gehen, um Blumen zu sehen\",\n]\n\nmodel = \"papluca/xlm-roberta-base-language-detection\"\npipe = pipeline(\"text-classification\", model=model)\npipe(text, top_k=1, truncation=True)\n</code></pre> <p>Result</p> <pre><code>[[{'label': 'en', 'score': 0.8889274001121521}],\n [{'label': 'ja', 'score': 0.9445705413818359}],\n [{'label': 'hi', 'score': 0.9658094048500061}],\n [{'label': 'es', 'score': 0.8788681030273438}],\n [{'label': 'el', 'score': 0.9942275285720825}],\n [{'label': 'fr', 'score': 0.9696151614189148}],\n [{'label': 'de', 'score': 0.9949468970298767}]]\n</code></pre>"},{"location":"languages/machine_translate/","title":"Machine Translation","text":"<p>Machine translation is the process of using artificial intelligence to automatically translate text from one language to another without human involvement. It translation goes beyond simple word-to-word translation to communicate the full meaning of the original language text in the target language. It analyzes all text elements not just grammetical words and recognizes how the words influence one another.</p>"},{"location":"languages/machine_translate/#approach","title":"\u27a4 Approach","text":"<p>In machine translation, the original text or language is called source language, and the language you want to translate it to is called the target language. Machine translation works by :</p> <ul> <li>Decode the source language meaning of the original text</li> <li>Encode the meaning into the target language</li> </ul> Machine Translation"},{"location":"languages/machine_translate/#rule-based-machine-translation","title":"\u27a4 Rule-Based Machine Translation","text":"<p>Rule-based machine translation relies on these resources to ensure precise translation of specific content. The process involves the software parsing input text, generating a transitional representation, and then converting it into the target language with reference to grammar rules and dictionaries.</p> <p>Language experts develop built-in linguistic rules and bilingual dictionaries for specific industries or topics. Rule-based machine translation uses these dictionaries to translate specific content accurately. </p> <p>Rule-based machine translation can be customized to a specific industry or topic. It is predictable and provides quality translation. However, it produces poor results if the source text has errors or uses words not present in the built-in dictionaries. The only way to improve it is by manually updating dictionaries regularly.</p>"},{"location":"languages/machine_translate/#statistical-machine-translation","title":"\u27a4 Statistical Machine Translation","text":"<p>Instead of relying on linguistic rules, statistical machine translation utilizes machine learning for text translation. Machine learning algorithms examine extensive human translations, identifying statistical patterns. When tasked with translating a new source text, the software intelligently guesses based on the statistical likelihood of specific words or phrases being associated with others in the target language.</p> <ul> <li>Syntax-based machine translation: Syntax-based machine translation is a sub-category of statistical machine translation. It uses grammatical rules to translate syntactic units. It analyzes sentences to incorporate syntax rules into statistical translation models.</li> </ul> <p>Statistical methods require training on millions of words for every language pair. However, with sufficient data the machine translations are accurate.</p>"},{"location":"languages/machine_translate/#neural-machine-translation-nmt","title":"\u27a4 Neural Machine Translation (NMT)","text":"<p>A neural network, inspired by the human brain, is a network of interconnected nodes functioning as an information system. Input data passes through these nodes to produce an output. Neural machine translation software utilizes neural networks to process vast datasets, with each node contributing a specific change from source text to target text until the final result is obtained at the output node.</p> <p>Neural machine translation software uses neural networks to work with enormous datasets. Each node makes one attributed change of source text to target text until the output node gives the final result.</p>"},{"location":"languages/machine_translate/#hybrid-machine-translation","title":"\u27a4 Hybrid Machine Translation","text":"<p>Hybrid machine translation tools integrate multiple machine translation models within a single software application, leveraging a combination of approaches to enhance the overall effectiveness of a singular translation model. This process typically involves the incorporation of rule-based and statistical machine translation subsystems, with the ultimate translation output being a synthesis of the results generated by each subsystem. You can use the hybrid approach to improve the effectiveness of a single translation model. </p>"},{"location":"languages/machine_translate/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>from transformers import pipeline\n\ntext = [\n    \"Brevity is the soul of wit.\",\n    \"I would like to go there\",\n    \"Did you see that car\",\n    \"good morning\",\n    \"There are 3 colors\",\n]\n\n# model to translate (english to french)\nmodel_checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\npipe = pipeline(\"text-classification\", model=model_ckpt)\ntranslator = pipeline(\"translation\", model=model_checkpoint)\ntranslator(text)\n</code></pre> <p>Result</p> <pre><code>[{'translation_text': \"La v\u00e9rit\u00e9 est l'\u00e2me de l'esprit.\"},\n {'translation_text': \"J'aimerais y aller.\"},\n {'translation_text': 'Avez-vous vu cette voiture ?'},\n {'translation_text': 'Bonjour.'},\n {'translation_text': 'Il y a 3 couleurs'}]\n</code></pre>"},{"location":"languages/pos/","title":"Part Of Speech: POS Tagging","text":"<p>Part-of-Speech (POS) tagging is a natural language processing technique that involves assigning specific grammatical categories or labels (such as nouns, verbs, adjectives, adverbs, pronouns, etc.) to individual words within a sentence. This process provides insights into the syntactic structure of the text, aiding in understanding word relationships, disambiguating word meanings, and facilitating various linguistic and computational analyses of textual data.</p> <p>POS tagging is essential for comprehending a language\u2019s syntactic structure, named entity recognition, information retrieval, and machine translation.</p> <p>POS are language dependent as diffrernt languages have different rules and grammers. Even though there are universal POS tagsets, it can be difficult to develop completely language-independent models because different languages have different rules and difficulties.</p>"},{"location":"languages/pos/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>from spacy import load\n# Load model \nnlp = load(\"en_core_web_sm\") \n\ntext = \"Tim Cook is the CEO of Apple company founded by Steve Jobs and Steve Wozniak in April 1976.\"\n\ndoc = nlp(text) \nfor token in doc: \n    print(token, token.pos_) \n\nprint(\"Verbs:\", [token.text for token in doc if token.pos_ == \"VERB\"])\n</code></pre> <p>Result</p> <pre><code>Tim PROPN\nCook PROPN\nis AUX\nthe DET\nCEO PROPN\nof ADP\nApple PROPN\ncompany NOUN\nfounded VERB\nby ADP\nSteve PROPN\nJobs PROPN\nand CCONJ\nSteve PROPN\nWozniak PROPN\nin ADP\nApril PROPN\n1976 NUM\n. PUNCT\nVerbs: ['founded']\n</code></pre>"},{"location":"nlp/evaluation_matrices/","title":"Evaluation Matrices : NLP","text":"<p>Evaluation metrics is used to optimize the model performance to identify the areas for improvement and make iterative adjustments to enhance the accuracy and precision of NLP systems.</p>"},{"location":"nlp/evaluation_matrices/#f1-score","title":"\u27a4 F1 score","text":"\\[ \\text{F1 Score}= 2\\times\\frac{Precision \\times Recall}{Precision + Recall} \\] <ul> <li>Precision = TP / (TP+FP)</li> <li> <p>Recall = TP / (TP+FN)</p> </li> <li> <p>F1 score commonly used in classification problems, is also applicable to various NLP tasks like Named Entity Recognition, POS-tagging, etc. The F1 score is the harmonic mean of precision and recall, and thus balances the two and prevents extreme cases where one is favored over the other. It ranges from 0 to 1, where 1 signifies perfect precision and recall.</p> </li> <li> <p>Precision is the number of true positive results divided by the number of all positive results, including those not identified correctly. Recall, on the other hand, is the number of true positive results divided by the number of all samples that should have been identified as positive.</p> </li> <li> <p>In NLP, F1 score is often used in Named Entity Recognition, POS-tagging, and other classification tasks. The F1 score is ideal when you need to balance precision and recall, especially in cases where both false positives and false negatives are equally costly.</p> </li> </ul>"},{"location":"nlp/evaluation_matrices/#perplexity","title":"\u27a4 Perplexity","text":"<p>For a probability distribution p and a sequence of N words w1,w2,...wN</p> \\[ Perplexity = \\sqrt[N]{\\frac{1}{p(w1, w2, .., wN)}} \\] <ul> <li> <p>Perplexity is a measure commonly used to assess how well a probability distribution predicts a sample. In the context of language models, it evaluates the uncertainty of a model in predicting the next word in a sequence.</p> </li> <li> <p>Perplexity serves as an inverse probability metric. A lower perplexity indicates that the model's predictions are closer to the actual outcomes, meaning the model is more confident (more accurate) in its predictions.</p> </li> <li> <p>Higher Perplexity (Human written): Suggests that the text is less predictable or more complex. In the context of language models, a higher perplexity might indicate that the model is less certain about its predictions or that the text has a more complex structure or vocabulary. </p> </li> <li> <p>Lower Perplexity (AI generated): Indicates that the text is more predictable or simpler. For language models, a lower perplexity usually means the model is more confident in its predictions and the text may follow more common linguistic patterns.</p> </li> </ul>"},{"location":"nlp/evaluation_matrices/#bertscore","title":"\u27a4 BERTScore","text":"<ul> <li> <p>BERTScore is used to evaluate the quality of text. It leverages the contextual embeddings from BERT. This allows for a more nuanced comparison of text, as BERTScore can understand the context in which words are used.</p> </li> <li> <p>It computes the cosine similarity between the embeddings of words in the candidate text and the reference text, accounting for the deep semantic similarity. The calculation involves finding the best match for each word in the candidate text within the reference text and averaging these scores.</p> </li> <li> <p>BERTScore leverages contextual embeddings, offering a sophisticated method to assess semantic similarity between generated and reference texts.</p> </li> </ul>"},{"location":"nlp/evaluation_matrices/#bleu-bilingual-evaluation-understudy","title":"\u27a4 BLEU (Bilingual Evaluation Understudy)","text":"\\[ BLEU = BP \\times\\exp(\\sum_{i=1}^{n} w_i \\times log(p_i)) \\] <ul> <li><code>BP</code>: brevity penalty (to penalize short sentences)</li> <li><code>w_i</code>: weights for each gram </li> <li> <p><code>p_i</code>: precision for each i-gram</p> </li> <li> <p>BLEU is predominantly used in machine translation. It quantifies the quality of the machine-generated text by comparing it with a set of reference translations. The crux of the BLEU score calculation is the precision of n-grams in the machine-translated text. However, to prevent the overestimation of precision due to shorter sentences, BLEU includes a brevity penalty factor. Note that BLEU mainly focuses on precision rather than recall.</p> </li> <li> <p>BLEU is effective in assessing the closeness of machine generated translations to a set of high quality reference translations. It's suitable when precision of translated text is a priority but it may not capture the fluency or grammatical correctness of the translation, as it focuses on the precision.</p> </li> </ul>"},{"location":"nlp/evaluation_matrices/#rouge-recall-oriented-understudy-for-gisting-evaluation","title":"\u27a4 ROUGE (Recall-Oriented Understudy for Gisting Evaluation)","text":"<ul> <li> <p>ROUGE is used for evaluating automatic summarization and machine translation. The key feature of ROUGE is its focus on recall, measuring how many of the reference n-grams are found in the system-generated summary. This makes it especially useful where coverage of key points is important. </p> </li> <li> <p>ROUGE-N computes the overlap of n-grams between the system and reference summaries.</p> </li> <li> <p>ROUGE-L uses the longest common subsequence to account for sentence-level structure similarity.</p> </li> <li> <p>ROUGE-S includes skip-bigram plus unigram-based co-occurrence statistics. Skip-bigram is any pair of words in their sentence order.</p> </li> </ul>"},{"location":"nlp/evaluation_matrices/#meteor-metric-for-evaluation-of-translation-with-explicit-ordering","title":"\u27a4 METEOR (Metric for Evaluation of Translation with Explicit ORdering)","text":"\\[ \\text{METEOR} = \\frac{10 \\bullet P \\bullet R}{R + 9 \\bullet P} \u2212 Penalty \\] <ul> <li><code>P</code>: precision (proportion of matched words in the machine translation)</li> <li><code>R</code>: recall (proportion of matched words in the reference translation)</li> <li> <p><code>Penalty</code>: for word order differences.</p> </li> <li> <p>METEOR is used for evaluating machine translation. Unlike BLEU, METEOR emphasizes both precision and recall, taking into account the number of matching words between the machine-generated text and reference translations. It's known for using synonyms and stemming to match words, allowing for a more flexible comparison.</p> </li> <li> <p>It calculates a score based on the harmonic mean of precision and recall, giving equal importance to both. It also includes a penalty for too many unmatched words, ensuring that translations are not just accurate but also coherent and fluent.</p> </li> </ul>"},{"location":"nlp/pii/","title":"PII Detection","text":"<p>PII stands for Personal Identifialbe Information. Any information that can uniquely identify people as individuals is called PII. Such as name, email, credit card number, social security number (usa), aadhaar number (india), fiscal code (italian), NIF number (spanish), driver license number, bank details, passport etc.</p> <p>PII contains sensitive and non sensitive information therefore it is good practice to encrypt them.</p> <p>For that we will used microsoft presidio a toolkit that provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.</p>"},{"location":"nlp/pii/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>With MS Presidio</p> <p>Installation </p> <pre><code>pip install presidio_analyzer presidio_anonymizer -q\npython -m spacy download en_core_web_lg -q\n</code></pre>"},{"location":"nlp/pii/#detect-pii","title":"Detect PII","text":"<p>Initialize</p> <pre><code>analyzer = AnalyzerEngine()\nengine = AnonymizerEngine()\nregistry = RecognizerRegistry()\n</code></pre> <pre><code>text = \"Mr. Alvin lives in Berlin and his id is 453612345678 with alvin@gmail.com at 4 AM\"\nanalyzer_results = analyzer.analyze(text=text, language=\"en\")\nprint(analyzer_results)\n</code></pre> <p>result</p> <pre><code>[type: EMAIL_ADDRESS, start: 58, end: 73, score: 1.0, type: PERSON, start: 4, end: 9, score: 0.85, type: LOCATION, start: 19, end: 25, score: 0.85, type: PERSON, start: 58, end: 73, score: 0.85, type: DATE_TIME, start: 77, end: 81, score: 0.85, type: URL, start: 64, end: 73, score: 0.5, type: US_BANK_NUMBER, start: 40, end: 52, score: 0.05, type: US_DRIVER_LICENSE, start: 40, end: 52, score: 0.01]\n</code></pre> <p>Get encrypted text</p> <pre><code>result = engine.anonymize(\n    text=text,\n    analyzer_results = analyzer_results,\n    operators={\"PERSON\": OperatorConfig(\"replace\", {\"new_value\": \"SECRET\"}), \n               \"DATE_TIME\":OperatorConfig(\"replace\", {\"new_value\": \"00 00 0000\"}),\n               \"EMAIL_ADDRESS\":OperatorConfig(\"replace\", {\"new_value\": \"dummay@email.com\"}),\n               \"LOCATION\":OperatorConfig(\"replace\", {\"new_value\": \"LOC\"}),\n               \"US_BANK_NUMBER\":OperatorConfig(\"replace\", {\"new_value\": \"BA12345\"}),\n               },\n)\n\nprint(result.text)\nprint(result.items)\n</code></pre> <p>result</p> <pre><code>text: Mr. SECRET lives in LOC and his id is BA12345 with dummay@email.com at 00 00 0000\nitems:\n[\n    {'start': 71, 'end': 81, 'entity_type': 'DATE_TIME', 'text': '00 00 0000', 'operator': 'replace'},\n    {'start': 51, 'end': 67, 'entity_type': 'EMAIL_ADDRESS', 'text': 'dummay@email.com', 'operator': 'replace'},\n    {'start': 38, 'end': 45, 'entity_type': 'US_BANK_NUMBER', 'text': 'BA12345', 'operator': 'replace'},\n    {'start': 20, 'end': 23, 'entity_type': 'LOCATION', 'text': 'LOC', 'operator': 'replace'},\n    {'start': 4, 'end': 10, 'entity_type': 'PERSON', 'text': 'SECRET', 'operator': 'replace'}\n]\n</code></pre>"},{"location":"nlp/spell_correction/","title":"Spell correction","text":"<p>Spell correction with dictionary based approach using pyspellchecker.</p>"},{"location":"nlp/spell_correction/#introduction","title":"Introduction","text":"<p>As we made many mistakes in typoes and writing correcting spelling mistakes is necessary  while texting a phone, sending an email, writing large documents or searching for information on the web.</p> <p>There are many approaches for correcting spelling. - Norvig's approach - Language model based - Phonetic matching based on pronunciation - Rule based approach with pattern matching - Dictionary based</p> <p>Let's implement using dictionary based approach with pyspellchecker.</p> <p>Pyspellchecker is based on Norvig\u2019s algorithm with a Levenshtein Distance algorithm to find permutations within an edit distance of 2 from the original word. It then compares all permutations (insertions, deletions, replacements, and transpositions) to known words in a word frequency list. Those words that are found more often in the frequency list are more likely the correct results.</p> <p>Pyspellchecker also supports (lang): en, es, fr, it, pt, de, ru, ar, lv, eu, nl </p> <p>\ud83d\udd17 pyspell docs</p>"},{"location":"nlp/spell_correction/#installation","title":"Installation","text":"<pre><code>pip install pyspellchecker\n</code></pre>"},{"location":"nlp/spell_correction/#create-a-pyspellchecker-object","title":"Create a pyspellchecker object","text":"<pre><code>from spellchecker import SpellChecker\ncorrect = SpellChecker()\n</code></pre> <p>Load word frequency dictionary <pre><code>correct.word_frequency.dictionary\n</code></pre></p> <p>word frequency dictionary  <pre><code>   { \"a\": 48779620,\n    \"aah\": 50,\n    \"aalii\": 50,\n    \"aardvark\": 106, ...}\n</code></pre></p>"},{"location":"nlp/spell_correction/#now-add-corpus-words-in-this-dictionary","title":"Now add corpus words in this dictionary","text":"<pre><code>with open('corpus.txt', 'r', encoding='utf-8') as rfile:\n    data = rfile.read()\nwords = list(set([word for word in data.split()])) # list of unique words\n# add this words to dict\ncorrect.word_frequency.load_words(words)\n</code></pre>"},{"location":"nlp/spell_correction/#load-a-dictionary-if-have-one-already","title":"Load a dictionary if have one already","text":"<pre><code>correct.word_frequency.load_dictionary('corpus.json')\n</code></pre>"},{"location":"nlp/spell_correction/#get-the-corrected-word","title":"Get the corrected word","text":"<pre><code>correct.correction('tomorow') # tommorrow\n</code></pre>"},{"location":"nlp/spell_correction/#get-the-suggestion-for-word","title":"Get the suggestion for word","text":"<pre><code>correct.candidates('wirld') # {'wild', 'wired', 'world', 'wield'}\n</code></pre>"},{"location":"nlp/spell_correction/#add-custom-word-to-dictinary","title":"Add custom word to dictinary","text":"<pre><code>correct.word_frequency.add('chococolato')\n</code></pre>"},{"location":"nlp/spell_correction/#remove-words-from-dictionary","title":"Remove words from dictionary","text":"<pre><code>correct.word_frequency.remove('chococolato')\ncorrect.word_frequency.remove_words(['apple', 'banana', 'berry']) # list\n</code></pre>"},{"location":"nlp/spell_correction/#remove-words-by-threshould-value","title":"Remove words by threshould value","text":"<p>Removes words at or below threshould value <pre><code># removes words having freq value 25 or below\ncorrect.word_frequency.remove_by_threshold(25)\n</code></pre></p>"},{"location":"nlp/spell_correction/#check-if-word-exists-in-dictionary-or-not","title":"Check if word exists in dictionary or not","text":"<pre><code>correct.known('apple')\n</code></pre>"},{"location":"nlp/spell_correction/#calculate-word-frequency-count","title":"Calculate word frequency count","text":"<pre><code>correct.word_usage_frequency(['apple'])\n</code></pre>"},{"location":"nlp/spell_correction/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>Now putting all this together</p> spell_correction.py<pre><code>from spellchecker import SpellChecker\nfrom pathlib import Path\nfrom typing import Union\n\nDIR = \"path/to/corpus\"\nclass Colors:\n    YELLOW = '\\033[33m'\n    ENDC = '\\033[m'\n\nclass CorrectSpell:\n    \"\"\" Custom spell correction with pyspellchecker.\n        docs: https://pyspellchecker.readthedocs.io/en/latest/index.html\n    \"\"\"\n    def __init__(self,\n                 corpus:Path,\n                 lang:str = \"en\"):\n        # supported lang(by pyspellchecker): [en, es, fr, it, pt, de, ru, ar, lv, eu, nl]\n        try:\n            self.correct = SpellChecker()\n        except ImportError:\n            print(\"!pip install pyspellchecker\")\n\n        self.corpus = corpus\n        self.load()\n\n    def load(self):\n        \"\"\" Load data with all frequency count.\n            corpus: data filepath to load (format: json or txt)\n        \"\"\"\n        if self.corpus.suffix == '.json': \n            self.correct.word_frequency.load_dictionary(self.corpus)\n\n        elif self.corpus.suffix == '.txt':\n            with open(self.corpus, 'r', encoding='utf-8') as rfile:\n                data = rfile.read()\n            words = list(set([word for word in data.split()])) # list of unique words\n            # add this words to dict\n            self.correct.word_frequency.load_words(words)\n            # self.correct.export('corpus_freq.gz')\n        else:\n            raise ValueError(\"Please provide valid file format!\")\n\n    def build_dict(self, store:Union[str, Path]=\"word_freq.json\"):\n        \"\"\" Build dict with words and freq count.\n            store (str, Path): filename/path to store\n        \"\"\"\n        import json\n        with open(store, 'w', encoding='utf-8') as wfile:\n            json.dump(self.correct.word_frequency.dictionary, wfile, sort_keys=True, indent=4)\n\n    def word_freq(self, words:Union[list[str]]):\n        \"\"\" Returns word frequency.\"\"\"\n        return [(w,self.correct.word_usage_frequency(w)) for w in words]\n\n    def add_to_dict(self, word:str):\n        \"\"\" Add words to existing dictionary.\n            word (str): word to add\n        \"\"\"\n        self.correct.word_frequency.add(word)\n\n    def remove_from_dict(self, word:Union[Union[str,list[str]],int]):\n        \"\"\" Add words to existing dictionary.\n            word (str, list): word or words to remove (int if wanted to remove from certain threshould)\n        \"\"\"\n        if isinstance(word, str):\n            # remove word from dict\n            self.correct.word_frequency.remove(word)\n\n        elif isinstance(word, list):\n            # remove list of words from dict\n            self.correct.word_frequency.remove_words(word)\n\n        elif isinstance(word, int):\n            # remove words from dict &lt;= threshold value\n            self.correct.word_frequency.remove_by_threshold(word)\n\n        else:\n            raise ValueError(\"Unable to remove!\")\n\n    def spell_corr(self, word:str):\n        \"\"\" Returns corrected word.\n            word (str): word to be corrected \n        \"\"\"\n        # correct the word if found in dict\n        corrected = self.correct.correction(word)\n        # set color highlighter for word (incorrected)\n        # that is not found or added in dict\n        word = str(Colors.YELLOW + word + Colors.ENDC)\n        return word if corrected is None else corrected\n\n    def check_word_in_dict(self, word):\n        \"\"\" Check if word is in dict or not.\"\"\"\n        return True if self.correct.known(word) else False\n\n    def suggestions(self, word:str):\n        \"\"\" Returns suggested words matched from dict.\n            word (str): get suggestions for word\n        \"\"\"\n        return self.correct.candidates(word)\n\n\nif __name__ == \"__main__\":\n    fpath = DIR.joinpath(\"spell_correction/corpus.txt\")\n    query = \"Tomorow i am ging to Disney parkkk at 10 o\\'clock !Yahooooo\"\n\n    c = CorrectSpell(fpath)\n    c.add_to_dict('Disney')\n    result = \" \".join([c.spell_corr(q) for q in query.split()])\n    print(f\"corrected: {result}\")\n\n    suggestions = c.suggestions('cheet')\n    print(f\"suggestions: {suggestions}\")\n</code></pre>"},{"location":"nlp/text_encoding/","title":"Text Encoding","text":"<p>Text encoding is a fundamental process to convert text into number or vector representation so as to preserve the context and relationship between words and sentences, such that a machine can understand the pattern associated in any text and can make out the context of sentences.</p> <p>As machines understands only numbers, these text data needs to converted into numerical data (hence text encoding is needed).</p> <p>Text encoding involves assigning a numerical value to each character/word in the text. These numerical values are then converted into binary code, which can be stored and processed by the computer. </p>"},{"location":"nlp/text_encoding/#bow","title":"\u27a4 BOW","text":"<p>Bag Of Words encodes sentences using the entire data corpus. It is a representation of text that describes the occurrence of words within a document. It is called a \"bag\" of words, because any information about the order or structure of words in the document is discarded. This only concerned with whether known words occur in the document, not where in the document. BOW is simple  and easy to implement, making it a good choice for basic text analysis tasks.</p> <p>Example</p> <pre><code>sentence = \"Odd number is a good number and numbers are good\"\n\no/p: [1, 2, 0, 0, 2, 0, 1, 0]\n</code></pre> <p>BOW does not capture word semantics or meaning. Also BOW struggles with out-of-vocabulary words that were not seen during training. So it is not good for tasks that requires deep semantic understanding.</p>"},{"location":"nlp/text_encoding/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>from nltk.corpus import stopwords\n\n# build vocab to store unique words\ndef vocab(text):\n    vocabulary = []\n    for w in word:\n        if w not in vocabulary:\n            vocabulary.append(w)\n    return vocabulary\n\ndef bow_encoding(text):\n    \"\"\" Bag of words.\"\"\"\n    word = text.split()\n    vocabulary = vocab(text)\n\n    # Initialize a vector of zeros with the same length as the vocabulary\n    bow_vector = [0] * len(vocabulary)\n    stop_words = set(stopwords.words('english'))\n    filtered_words = [w for w in word if w.lower() not in stop_words]\n    word_to_index = {word: idx for idx, word in enumerate(vocabulary)}\n\n    for word in filtered_words:        \n        if word in word_to_index:\n            bow_vector[word_to_index[word]] += 1\n    return bow_vector\n\ntext = \"Some input text\"\n</code></pre>"},{"location":"nlp/text_encoding/#one-hot-encoding","title":"\u27a4 One hot encoding","text":"<p>One hot encoding converts text data into categorical data, such as words or the characters into a binary vector representation. Each unique category (word or character) is represented by a binary vector where only one element is set to 1, while all others are set to 0.</p> <p>First text data needs to be tokenized an afterwords build a vocabulary that contains all unique words or tokens present in your corpus. For each word or token in your vocabulary, create a binary vector of the same length as the vocabulary size. In this binary vector, only one element is set to 1, representing the position of the word in the vocabulary.</p> <p>Example</p> <p>sentence = \"Odd number is a good number and numbers are good\"</p> <pre><code>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],    # Odd(1)\n[0, 1, 0, 0, 0, 1, 0, 0, 0, 0],     # number(2) number(6)\n[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],     # is(3)\n[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],     # a(4)\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 1],     # good(5) good(10)\n[0, 1, 0, 0, 0, 1, 0, 0, 0, 0],     # number(2) number(6)\n[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],     # and(7)\n[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],     # numbers(8)\n[0, 0, 0, 0, 0, 0, 0, 0, 1, 0],     # are(9)\n[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]]     # good(5) good(10)\n</code></pre> <p>It is straightforward and easy to implement approach and each representation is independent of each other words. But the encoded vectors can be very high dimensional, especially for large vocabularies, which can lead to computational inefficiency and memory usage.</p>"},{"location":"nlp/text_encoding/#implementation_1","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>def one_hot_encoding(text):\n    \"\"\" One hot encoding.\"\"\"\n    word = text.split()\n    encoded_text = []\n    for each in word:\n        one_hot_encoded = [1 if word == each else 0 for word in word]\n        encoded_text.append(one_hot_encoded)\n    return encoded_text\n</code></pre>"},{"location":"nlp/text_encoding/#index-based","title":"\u27a4 Index based","text":"<p>As in name, The Index based encoding involves assigning a unique index to each token in the text, allowing it to uniquely identify and represent all tokens within the corpus by their respective index values.</p> <p>Example</p> <p>sentence = \"Odd number is a good number and numbers are good\"</p> <p>By assigning each token a unique index</p> <pre><code>{\n    \"Odd\": 1,\n    \"number\": 2,\n    \"is\": 3,\n    \"a\": 4,\n    \"good\":5, \n    \"number\":2,\n    \"and\": 6,\n    \"numbers\": 7,\n    \"are\": 8,\n    \"good\": 5\n}\n</code></pre> <p>Now we have encoded all the words with index numbers, it is now machine reable and this can be used as input.</p> <p>Index based are easy to use as words converted/assigned to unique index value but for using this encoding make sure that all sentences are equally long. By that means if the first sentence has 4 words but the last sentence has 5 words, this will cause an imbalance.</p> <p>So this issue max padding is used, which takes the longest sentence from document corpus and pad the other sentence to be as long. This means if all sentences are of 4 words and one sentence is of 5 words max padding will make all the sentences of 5 words by using extra index 0.</p> <p>\"[1, 2, 3, 4, 0]\" </p> <p>\"[5, 6, 7, 2, 0]\" </p> <p>\"[4, 8, 9, 10, 0]\" </p> <p>\"[11, 12, 2, 5, 13]\"</p> <p>Index-Based Encoding considers the sequence information in text encoding. Also index-based encoding results in high-dimensional sparse vectors where all words are treated as independent entities and there is no similarity information encoded in the vectors.</p>"},{"location":"nlp/text_encoding/#implementation_2","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>def index_based_encoding(text):\n    \"\"\" Index based encoding.\"\"\"\n    word = text.split()\n    encoded_corpus = []\n    encoded_sentence = [word_to_index[w]+1 for w in word]\n    encoded_corpus.append(encoded_sentence)\n    return encoded_corpus  \n</code></pre>"},{"location":"nlp/text_encoding/#tf-idf","title":"\u27a4 TF-IDF","text":"<p>TFIDF known as Term Frequency-Inverse Document Frequency. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus.</p> <p>Let's see the terminology for better understanding.</p> <p>Term Frequency (TF) </p> <p>This component measures how frequently a word appears in a document. It gives higher weight to words that occur more frequently in a document. The number of times a term occurs in a document is called its term frequency. The weight of a term that occurs in a document is simply proportional to the term frequency.</p> \\[     tf(t,d) = \\frac{\\text{count of t in d}}{\\text{number of words in d}} \\] <p>Document Frequency (DF) </p> <p>Document frequency measures the importance of a document in a whole set of corpus. This is very similar to TF. The only difference is that TF is a frequency counter for a term t in document d, whereas DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once. We don\u2019t need to know the number of times the term is present.</p> \\[     df(t) = \\text{occurrence of t in documents} \\] <p>Inverse Document Frequency (IDF) </p> <p>IDF measures the importance of a word in the entire corpus by considering how many documents contain the word. Words that are common across many documents receive lower IDF scores, while words that are rare or unique to a few documents receive higher scores.</p> \\[     idf(t) = log(N/ df(t)) \\] <p>where</p> <p>df(t) = Document frequency of a term t </p> <p>N(t) = Number of documents containing the term t</p>"},{"location":"nlp/text_encoding/#implementation_3","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>def tfidf_encoding(text):\n    \"\"\" Term Frequency Inverse Document Frequency. \"\"\"\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    from pandas import DataFrame\n\n    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n    # tfidf_matrix : (doc_is, term_index) score/value -- generates csr_matrix\n    feature_names = tfidf_vectorizer.get_feature_names_out()\n    print(f\"{feature_names=}\")\n\n    tfidf_df = DataFrame(data=tfidf_matrix.toarray(), columns=feature_names)\n    return tfidf_df, tfidf_matrix\n</code></pre>"},{"location":"nlp/text_encoding/#word2vec","title":"\u27a4 Word2vec","text":"<p>Word2Vec is a popular nlp technique for generating word embedding, which are vector representations of words in a continuous vector space. Word embedding capture semantic relationships between words, making them suitable for various nlp tasks like language modeling, sentiment analysis, and machine translation. Word2Vec has two main algorithms: Continuous Bag of Words (CBOW) and Skip-gram.</p> <p>The word2vec tool takes a text corpus as input and produces the word vectors as output. It first constructs a vocabulary from the training text data and then learns vector representation of words. The resulting word vector file can be used as features in many natural language processing and machine learning applications.</p> <p>Word2Vec consists of a shallow neural network with a single hidden layer. The input and output layers have a vocabulary size equal to the number of unique words in the corpus. The hidden layer contains the word embedding.</p> CBOW and Skipgram <p>CBOW We have to predict target word using context words. Several parameters such as window size, vocab size, etc are defined according to the problem domain. One hot encoding is done for each word input. The output vector (Embedding Dimension) for each word nothing but the weight value of hidden layer to the outer layer.</p> <p>Skip-Gram We have to predict context words using target word. Here also several parameters defined. Architectures is exact opposite as CBOW. The output vector (Embedding Dimension) for each word nothing but the weight value of first input layer to the hidden layer.</p>"},{"location":"nlp/text_encoding/#implementation_4","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<pre><code>def word2vec_encoding(text, term1='', term2='', model=\"en_core_web_sm\"):\n    \"\"\" Word2vec. \"\"\"\n    from spacy import load, info\n    # print(info()['pipelines'].keys()) # list out installed model\n    spacy_model = load(model)\n    doc = spacy_model(text)\n    match_term = spacy_model(term2)\n\n    similarity = [token.similarity(term2) for token in doc for term2 in match_term if token.text==term1]\n    return similarity\n</code></pre>"},{"location":"nlp/text_preprocessing/","title":"Text Preprocessing","text":"<p>Text preprocessing is an essential first step in natural language processing (nlp) that involves cleaning and transforming unstructured text data to prepare it for analysis. It includes tokenization, stemming, lemmatization, stop-word removal.</p> <p>Text preprocessing steps for a problem depend mainly on the domain and the problem itself so it can be vary depend on requirements.</p> <p>Text data often contains punctuation, emojis, special characters, and fuzzy symbols. These noisy data must be removed for model building. Preprocessing helps remove these elements, making the text cleaner and easier to analyze.</p>"},{"location":"nlp/text_preprocessing/#preprocessing","title":"Preprocessing","text":""},{"location":"nlp/text_preprocessing/#convert-to-lowercase","title":"Convert to lowercase","text":"<p>Text lowercase helps to reduce the size of the vocabulary and also complexity of our text data.</p> <pre><code>text = \"Do TEXT processing in Natural Language Processing\"\ntext.lower()\n</code></pre>"},{"location":"nlp/text_preprocessing/#remove-stop-words","title":"Remove stop words","text":"<p>Stopwords like do, an, him, it, did, so, wasn't, had, once are words that do not contribute to the meaning of a sentence. Hence, they can safely be removed without causing any change in the meaning of the sentence. The NLTK library has a set of stopwords and we can use these to remove stopwords from our text and return a list of word tokens. But it is not necessary to use the provided list as stopwords as they should be chosen differently based on the requirements.</p> <pre><code>from nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n# add custom stop words to remove\nstop_words.add('hello')\n</code></pre>"},{"location":"nlp/text_preprocessing/#tokenization","title":"Tokenization","text":"<p>Split the text into smaller units.</p> <pre><code>from nltk.tokenize import word_tokenize\nword_tokenize(text)\n</code></pre>"},{"location":"nlp/text_preprocessing/#remove-matched-expression","title":"Remove matched expression","text":"<p>Remove numbers, punctuations or any other particular occurance using regex expression.</p> <pre><code>import re\nexpression = '\\d+' # digits\nremove_digit = re.sub(expression, '', text)\n</code></pre>"},{"location":"nlp/text_preprocessing/#stemming","title":"Stemming","text":"<p>Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.</p> <ul> <li>books      ---&gt;    book</li> <li>looked     ---&gt;    look</li> <li>denied     ---&gt;    deni</li> <li>science    ---&gt;    scienc</li> </ul> <pre><code>from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\nprint([porter_stemmer.stem(word) for word in text.split()])\n</code></pre>"},{"location":"nlp/text_preprocessing/#lemmatization","title":"Lemmatization","text":"<p>Like stemming, lemmatization also converts a word to its root form. The only difference is that lemmatization ensures that the root word belongs to the language. We will get valid words if we use lemmatization. In NLTK, we use the WordNetLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization. So, we add the part-of-speech as a parameter. </p> <ul> <li>goose    --&gt;   goose</li> <li>geese    --&gt;   goose</li> <li>science  --&gt;   science</li> </ul> <pre><code>from nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nprint([wordnet_lemmatizer.lemmatize(word) for word in text.split()])\n</code></pre>"},{"location":"parser/html_parser/","title":"HTML Parser","text":"<p>As we know Browser understands HTML. So web data is in Hyper Text Markup Language which has a tree-like data structure that powers the web. To get data from web documents parsing is needed.</p>"},{"location":"parser/html_parser/#html-structure","title":"HTML structure","text":"<p>HTML DOM structure</p> <pre><code>graph TB\n  A{{Document}} --&gt; |root| B[html];\n  B --&gt; |Element| C[head];\n  B --&gt; |Element| D[body];\n  C --&gt; |Element| E[title];\n  E --&gt; |text| F(\"My NLP Roadmap\");\n  D --&gt; |Element| G[a];\n  D --&gt; |Element| H[p];\n  G --&gt; |attr| I([href]);\n  G --&gt; |text| J(http://nlp.com);\n  H --&gt; |text| K(Natural Language Processing);\n</code></pre>"},{"location":"parser/html_parser/#parse-html","title":"Parse HTML","text":"<p>Parse html with beautifulsoup </p> <p>Install  <pre><code>pip install beautifulsoup4\n</code></pre></p>"},{"location":"parser/html_parser/#get-html-content-from-url","title":"get html content from url","text":"<pre><code>from urllib.request import urlopen\n\nwith urlopen('https://docs.python.org/3/library/urllib.request.html') as r:\n    html_text = r.read().decode('utf-8')\n</code></pre>"},{"location":"parser/html_parser/#get-title-content","title":"get title content","text":"<pre><code>from bs4 import BeautifulSoup\n\n# make object\nsoup = BeautifulSoup(html_text, 'html.parser')\nprint(soup.title.string)\n# urllib.request \u2014 Extensible library for opening URLs \u2014 Python 3.12.3 documentation\n</code></pre>"},{"location":"parser/html_parser/#get-data-with-removed-spaced","title":"get data with removed spaced","text":"<pre><code>fetch = soup.head.stripped_strings\nprint([repr(f) for f in fetch])\n# [\"'urllib.request \u2014 Extensible library for opening URLs \u2014 Python 3.12.3 documentation'\"]\n</code></pre>"},{"location":"parser/html_parser/#get-parent-tag-data","title":"get parent tag data","text":"<pre><code>fetch = soup.a\nprint([f.name for f in fetch.parent])\n</code></pre>"},{"location":"parser/html_parser/#get-siblings-tag-data","title":"get siblings tag data","text":"<pre><code>fetch = soup.a\nprint([repr(f) for f in fetch.next_siblings]) \nprint([repr(f) for f in fetch.previous_siblings]) \n</code></pre>"},{"location":"parser/html_parser/#find-tag-data-with-id","title":"find tag data with id","text":"<pre><code>fetch = soup.find(\"a\", id=\"reference internal\")\nprint(fetch.text)\n# urllib.request \u2014 Extensible library for opening URLs\n\nfetch = soup.find_all(\"a\", id=\"reference internal\", limit=5)\nprint([f.text for f in fetch])\n# ['urllib.request \u2014 Extensible library for opening URLs', 'Request Objects', 'OpenerDirector Objects', 'BaseHandler Objects', 'HTTPRedirectHandler Objects']\n</code></pre>"},{"location":"parser/html_parser/#find-with-string-match","title":"find with string match","text":"<pre><code>text = 'must be an'\nsoup.find_all(string=re.compile(text), limit=2)\n# [' must be an object specifying ...', ' must be an object ... ']\n</code></pre>"},{"location":"parser/json_parser/","title":"JSON Parser","text":"<p>JSON  is a lightweight data-interchange format which is easy to read and write.</p> <p>JSON is built on two structures:</p> <ol> <li> <p>A collection of name/value pairs. In various languages, this is realized as an object, record, struct, dictionary, hash table, keyed list, or associative array.</p> </li> <li> <p>An ordered list of values. In most languages, this is realized as an array, vector, list, or sequence.</p> </li> </ol> <p>JSON is very popular format for data exchange between web applications and APIs.</p> <p>Json's text-based format is used to store data in a key-value format which is very similar to python dictionaries and javaScript objects.</p>"},{"location":"parser/json_parser/#parse-json","title":"Parse Json","text":"<p>Json data is readble but not searchable!</p> <p>As they made up of many many nested layers with lots of keys and arrays. It's not uncommon to see Json datasets with hundreds of keys and values so it becomes very complex.</p> <p>Therefore parsing is needed to make data accessability faster and more flexible.</p>"},{"location":"parser/json_parser/#nested-lookup","title":"Nested-lookup","text":"<p>The\u00a0nested_lookup\u00a0package provides many Python functions for working with deeply nested documents. A document in this case is a a mixture of Python dictionary and list objects typically derived from YAML or JSON.</p> <p>sample data <pre><code>data = {\n    \"product\":[{\n        \"T-shirt\":{\n            \"property\": [\n                {\"size\": \"L\", \"color\": \"blue\"},\n                {\"size\": \"S\", \"color\": \"pink\"},\n                {\"size\": \"M\", \"color\": \"black\"}\n                ],\n            \"category\":[\n                {\"officewear\": [\n                    {\"type\": \"polo shirt\", \"price\": 100}\n                    ]\n                },\n                {\"casualwear\":[\n                    {\"type\": \"long sleeves\", \"price\": 200}\n                ]}\n            ]\n        },\n        \"Dress\":{\n            \"property\":[\n                {\"color\":\"red\"},\n                {\"color\":\"white\"}\n            ]\n        }\n    }]\n} \n</code></pre></p> <p>Install </p> <pre><code>pip install nested-lookup \n</code></pre>"},{"location":"parser/json_parser/#lookup","title":"lookup","text":"<p><code>nested_lookup</code> returns list of matched key lookups <pre><code>from nested_lookup import nested_lookup\n\nprint(nested_lookup('color', data))\n# ['blue', 'pink', 'black', 'red', 'white']\n</code></pre></p>"},{"location":"parser/json_parser/#update","title":"update","text":"<p><code>nested_update</code> returns all occurences of the given key and update the value. By default, returns a copy of the document. To mutate the original specify the in_place=True argument.</p> <pre><code>from nested_lookup import nested_update\n\n# before\nprint(nested_lookup('price', data)) # [100, 200]\n\nnested_update(data, 'price', 300, in_place=True)\n\n# after\nprint(nested_lookup('price', data)) # [300, 300]\n</code></pre>"},{"location":"parser/json_parser/#delete","title":"delete","text":"<p><code>nested_delete</code> returns all occurrences of the given key and delete it. By default, returns a copy of the document. To mutate the original specify the in_place=True argument.</p> <pre><code>from nested_lookup import nested_delete\n# before\nprint(nested_lookup('price', data)) # [100, 200]\n\nnested_delete(data, 'price', in_place=True)\n\n# after\nprint(nested_lookup('price', data)) # []\n</code></pre>"},{"location":"parser/json_parser/#alter","title":"alter","text":"<p><code>nested_alter</code> returns all occurrences of the given key and alter it with a callback function. By default, returns a copy of the document. To mutate the original specify the in_place=True argument.</p> <pre><code>from nested_lookup import nested_alter\n# before\nprint(nested_lookup('price', data)) # [100, 200]\n\ndef callback_to_alter(self, something:int):\n    return something + 500\n\nnested_alter(data, 'price', callback_to_alter, in_place=True)\n\n# after\nprint(nested_lookup('price', data)) # [600, 700]\n</code></pre>"},{"location":"parser/json_parser/#get-keys","title":"get keys","text":"<p><code>get_all_keys</code> returns a list of keys</p> <pre><code>from nested_lookup import get_all_keys\nget_all_keys(data)\n# ['product', 'T-shirt', 'property', 'size', 'color', 'size', 'color', 'size', 'color', 'category', 'officewear', 'type', 'price', 'casualwear', 'type', 'price', 'Dress', 'property', 'color', 'color']\n</code></pre>"},{"location":"parser/json_parser/#get-occurrences","title":"get occurrences","text":"<p><code>get_occurrence_of_key</code> and <code>get_occurrence_of_value</code> returns the number of occurrences of a key/value from a nested dictionary</p> <pre><code>from nested_lookup import get_occurrence_of_key, get_occurrence_of_value\n\nkey_occurence = get_occurrence_of_key(data, key=\"color\") # 5\nval_occurence = get_occurrence_of_value(data, value=\"red\") # 1\n</code></pre>"},{"location":"parser/json_parser/#jsonpath","title":"Jsonpath","text":"<p>This library differs from other JSONPath implementations in that it is a full\u00a0language\u00a0implementation, meaning the JSONPath expressions are first class objects, easy to analyze, transform, parse, print, and extend.</p> <p>Install </p> <pre><code>pip install jsonpath-ng \n</code></pre> <pre><code>from jsonpath_ng import parse\nfrom jsonpath_ng.jsonpath import Fields, Slice\n\nexpression = 'product[*].Dress.property[*].color'\n# convert custom_path to str to pass as expression \n# ex: parse(str(custom_path))\ncustom_path = Fields('product').child(Slice('*')).child(Fields('Dress'))\n\nparsed_data = parse(expression)\nfor match in parsed_data.find(data):\n    print(match.value)\n    print(str(match.full_path))\n\n# match_value: ['red', 'white']\n# match_full_path: ['product.[0].Dress.property.[0].color', 'product.[0].Dress.property.[1].color']\n</code></pre>"},{"location":"parser/parser/","title":"Parser","text":"<p>The word pars means \u2018parts of speech\u2019, from Old French pars \u2018parts\u2019 (influenced by Latin pars \u2018part\u2019). Parser is to analyse (a string or text) into logical syntactic components.</p>"},{"location":"parser/parser/#introduction","title":"Introduction","text":"<p>Parser helps to detect syntax errors, create a parse tree and symbol table or generate Intermediate Representations(IR).</p> <p>There are many different ways to categorize sentence structures. One way is to classify the words by grouping them as a single unit or phrase, which is also known as a constituent.</p> <p>Different language has different rules and structures. But there are common grammer rules that are of a noun phrase, verb phrase, and prepositional phrase structured as follows:</p> <p>Sentence = S = Noun Phrase + Verb Phrase + Preposition Phrase </p> <p>S = NP + VP + PP</p> <p>The different word groups that exist according to English grammar rules are:</p> <p>Noun Phrase(NP): Determiner + Nominal Nouns = DET + Nominal </p> <p>Verb Phrase (VP): Verb + range of combinations </p> <p>Prepositional Phrase (PP): Preposition + Noun Phrase = P + NP </p> <p>EX : the  boy (Noun) ate (verb) the pancakes (Noun).</p>"},{"location":"parser/parser/#nltk","title":"NLTK","text":"<p>NLTK includes pre-trained Punkt tokenizer which is used to tokenize the words  and averaged_perceptron_tagger which is used to tag those tokenized words to Parts of Speech.</p>"},{"location":"parser/parser/#pos-tags-in-nltk","title":"POS tags in nltk","text":"<pre><code>import nltk\nnltk.download('tagsets')\nnltk.help.upenn_tagsets('DT')\n</code></pre> Tag Meaning <code>CC</code> coordinating conjunction <code>CD</code> cardinal digit <code>DT</code> determiner <code>EX</code> existential there (like: \u201cthere is\u201d \u2026 think of it like \u201cthere exists\u201d) <code>FW</code> foreign word <code>IN</code> preposition/subordinating conjunction <code>JJ</code> adjective \u2018big\u2019 <code>JJR</code> adjective, comparative \u2018bigger\u2019 <code>JJS</code> adjective, superlative \u2018biggest\u2019 <code>LS</code> list marker <code>MD</code> modal could, will <code>NN</code> noun, singular \u2018desk\u2019 <code>NNS</code> noun plural \u2018desks\u2019 <code>NNP</code> proper noun, singular \u2018Harrison\u2019 <code>NNPS</code> proper noun, plural \u2018Americans\u2019 <code>PDT</code> predeterminer \u2018all the kids\u2019 <code>POS</code> possessive ending parent\u2019s <code>PRP</code> personal pronoun I, he, she <code>PRP$</code> possessive pronoun my, his, hers <code>RB</code> adverb very, silently, <code>RBR</code> adverb, comparative better <code>RBS</code> adverb, superlative best <code>RP</code> particle give up <code>TO,</code> to go \u2018to\u2019 the store. <code>UH</code> interjection, errrrrrrrm <code>VB</code> verb, base form take <code>VBD</code> verb, past tense took <code>VBG</code> verb, gerund/present participle taking <code>VBN</code> verb, past participle taken <code>VBP</code> verb, sing. present, non-3d take <code>VBZ</code> verb, 3rd person sing. present takes <code>WDT</code> wh-determiner which <code>WP</code> wh-pronoun who, what <code>WP$</code> possessive wh-pronoun whose <code>WRB</code> wh-abverb where, when"},{"location":"parser/parser/#pos-tagwords","title":"POS tagwords","text":"<pre><code>nltk.corpus.brown.tagged_words(tagset='universal')\n</code></pre> Tag Example <code>ADJ</code> adjective   (new, good, high, special, big, local) <code>ADP</code> adposition  (on, of, at, with, by, into, under) <code>ADV</code> adverb (really, already, still, early, now) <code>CONJ</code> conjunction (and, or, but, if, while, although) <code>DET</code> determiner, article (the, a, some, most, every, no, which) <code>NOUN</code> noun (year, home, costs, time, Africa) <code>NUM</code> numeral (twenty-four, fourth, 1991, 14:24) <code>PRT</code> particle (at, on, out, over per, that, up, ith) <code>PRON</code> pronoun (he, their, her, its, my, I, us) <code>VERB</code> verb (is, say, told, given, playing, would) <code>.</code> punctuation marks (. , ; !) <code>X</code> other (ersatz, esprit, dunno, gr8, univeristy)"},{"location":"parser/parser/#information-extraction","title":"Information Extraction","text":"Information Extraction System Architecture (https://www.nltk.org/book_1ed/ch07.html) <p>Information extraction is the very first and important step in NLP. Let's see more in detail.</p> <p><pre><code>def preprocess(document):\n    sentences = nltk.sent_tokenize(document) # sentence segmenter\n    tokens = [nltk.word_tokenize(sent) for sent in sentences] # word tokenizer\n    pos = [nltk.pos_tag(sent) for sent in tokens] # pos\n    return sentences, tokens, pos\n\nsentence = \"Well! If Alex can come by 4 o\\'clock at Mall.\"\nsegment, token, pos = preprocess(sentence)\n</code></pre> gives</p> <pre><code>segment=['Well!', \"If Alex can come by 4 o'clock at Mall.\"]\ntoken=[['Well', '!'], ['If', 'Alex', 'can', 'come', 'by', '4', \"o'clock\", 'at', 'Mall', '.']]\npos=[[('Well', 'RB'), ('!', '.')], [('If', 'IN'), ('Alex', 'NNP'), ('can', 'MD'), ('come', 'VB'), ('by', 'IN'), ('4', 'CD'), (\"o'clock\", 'NN'), ('at', 'IN'), ('Mall', 'NNP'), ('.', '.')]]\n</code></pre>"},{"location":"parser/parser/#chuncking","title":"Chuncking","text":"<p>Chunking, which segments and labels multi-token sequences as illustrated below. The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level chunking. Each of these larger boxes is called a chunk. Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text.</p> Segmentation and Labeling at both the Token and Chunk Levels (https://www.nltk.org/book_1ed/ch07.html) <p>Example <pre><code>def regex_based_chunks(sentence):\n    # rule: NP chunk should be formed whenever the \n    # chunker finds an optional determiner (DT) followed \n    # by any number of adjectives (JJ) and then a noun (NN)\n    grammar = \"NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}\" \n    # create chunk parser\n    cp = nltk.RegexpParser(grammar) \n    result = cp.parse(sentence) \n    return result\n\n# chunk structures must contain tagged tokens or trees \npos = [('Well', 'RB'), ('!', '.'), ('If', 'IN'), ('Alex', 'NNP'), ('can', 'MD'), ('come', 'VB'), ('by', 'IN'), ('4', 'CD'), (\"o'clock\", 'NN'), ('at', 'IN'), ('Mall', 'NNP'), ('.', '.')] \nregex_based_chunks(pos)\n</code></pre> gives <pre><code>chunks=Tree('S', \n            [('Well', 'RB'), ('!', '.'), ('If', 'IN'), ('Alex', 'NNP'), ('can', 'MD'), ('come', 'VB'), ('by', 'IN'), ('4', 'CD'), \n            Tree('NP', \n                [(\"o'clock\", 'NN')]), ('at', 'IN'), ('Mall', 'NNP'), ('.', '.')])\n</code></pre></p>"},{"location":"parser/parser/#using-spacy","title":"Using Spacy","text":"<pre><code>import spacy\n# Loading the model\nnlp=spacy.load('en_core_web_sm')\ntext = \"Tim Cook is the CEO of Apple company founded by Steve Jobs and Steve Wozniak in April 1976.\"\n\ndoc=nlp(text)\nfor token in doc:\n    print(token.text,'--&gt;',token.dep_)\n\n# result\nTim --&gt; compound\nCook --&gt; nsubj\nis --&gt; ROOT\nthe --&gt; det\nCEO --&gt; attr\nof --&gt; prep\nApple --&gt; compound\ncompany --&gt; pobj\nfounded --&gt; acl\nby --&gt; agent\nSteve --&gt; compound\nJobs --&gt; pobj\nand --&gt; cc\nSteve --&gt; compound\nWozniak --&gt; conj\nin --&gt; prep\nApril --&gt; pobj\n1976 --&gt; nummod\n. --&gt; punct\n</code></pre>"},{"location":"resources/resources/","title":"NLP Learning Resources","text":"<ol> <li>CS224N: Natural Language Processing with Deep Learning</li> <li>Dive into deep learning:NLP</li> <li>Huggingface NLP Course</li> <li>Transformers from scratch - by Peter bloem</li> <li>Annotated Transformer</li> <li>NLP resources</li> <li>Andrej Karpathy's NN series</li> </ol>"},{"location":"resources/resources/#blogs","title":"\u27a4 Blogs","text":"<ul> <li>Sebastian Raschka - Ahead of AI</li> <li>Chip Huyen Blog</li> <li>Eugeneyan Blog</li> <li>Lilian Weng's Log</li> </ul>"},{"location":"sentiment_analysis/sentiment_analysis/","title":"Sentiment Analysis","text":"<p>Sentiment analysis is the process of analyzing text to determine if the emotional tone of the message is positive, negative, or neutral. Today, companies have large volumes of text data like emails, customer support chat transcripts, social media comments, and reviews. Sentiment analysis tools can scan this text to automatically determine the customer attitude towards a product. Companies use the insights from sentiment analysis to improve customer service and increase brand reputation.</p>"},{"location":"sentiment_analysis/sentiment_analysis/#sentiment-analysis-in-real-world","title":"\u27a4 Sentiment analysis in real world","text":"<p>For instance, consider a movie reviews.</p> <p>Example</p> <p>User Review 1: \u201cI love this movie, it's fun to watch\u201d --&gt; positive. </p> <p>User Review 2: \u201cI have never seen a worst movie like these\u201d --&gt; negative.</p> <p>User Review 3: \u201cI watch this movie\u201d --&gt; neutral.</p> <p>As there are lots of data, a sentiment analysis model is crucial for identifying patterns in user reviews, as initial customer preferences may lead to a skewed perception of positive feedback. By processing a large corpus of user reviews, the model provides substantial evidence, allowing for more accurate conclusions than assumptions from a small sample of data.</p> <p>\u27a4 Use cases</p> <ul> <li>Social Media Monitoring </li> </ul> <p>As businesses are growing with social media, it allows businesses to gain insights about how customers feel about their product and services or to have any issues. </p> <ul> <li>Brand Monitoring</li> </ul> <p>Brand monitoring offers a wealth of insights from conversations happening about your brand from all over the internet like news articles, blogs, forums, and more to gauge brand sentiment, and target certain demographics or regions, as desired. Understanding customer feelings and opinions about how your brand image evolves over time.</p> <ul> <li>Voice of customer (VoC)</li> </ul> <p>Open-ended survey responses can be classified into positive and negative (and everywhere in between) offering further insights on customer support interactions, to understand the emotions and opinions of your customers. Tracking customer sentiment over time adds depth to help understand individual aspects of your business.</p> <ul> <li>Customer Service</li> </ul> <p>Sentiment analysis can be used to automatically organize incoming support queries by topic and urgency to route them to the correct department and make sure the most urgent are handled right away.</p> <ul> <li>Market Research</li> </ul> <p>To analyze online reviews of company products and compare them to market  competition. Uncover trends just as they emerge, or follow long-term market leanings through analysis of formal market reports and business journals.</p>"},{"location":"sentiment_analysis/sentiment_analysis/#types-of-sentiment-analysis","title":"\u27a4 Types of Sentiment  Analysis","text":"<ul> <li> <p>Fine-grained scoring</p> <p>Fine-grained sentiment analysis refers to categorizing the text intent into multiple levels of emotion. Typically, the method involves rating user sentiment on a scale of 0 to 100, with each equal segment representing very positive, positive, neutral, negative, and very negative. Ecommerce stores use a 5-star rating system as a fine-grained scoring method to gauge purchase experience. </p> </li> <li> <p>Aspect-based</p> <p>Aspect-based analysis focuses on particular aspects of a product or service. For example, laptop manufacturers survey customers on their experience with sound, graphics, keyboard, and touchpad. They use sentiment analysis tools to connect customer intent with hardware-related keywords. </p> </li> <li> <p>Intent-based</p> <p>Intent-based analysis helps understand customer sentiment when conducting market research. Marketers use opinion mining to understand the position of a specific group of customers in the purchase cycle. They run targeted campaigns on customers interested in buying after picking up words like discounts, deals, and reviews in monitored conversations. </p> </li> <li> <p>Emotional detection</p> <p>Emotional detection involves analyzing the psychological state of a person when they are writing the text. Emotional detection is a more complex discipline of sentiment analysis, as it goes deeper than merely sorting into categories. In this approach, sentiment analysis models attempt to interpret various emotions, such as joy, anger, sadness, and regret, through the person's choice of words. </p> </li> </ul>"},{"location":"sentiment_analysis/sentiment_analysis/#importance-of-sentiment-analysis","title":"\u27a4 Importance of Sentiment  Analysis","text":"<ul> <li> <p>Provide objective insights</p> <p>Businesses can avoid personal bias associated with human reviewers by using artificial intelligence (AI)\u2013based sentiment analysis tools. As a result, companies get consistent and objective results when analyzing customer's opinions.</p> </li> <li> <p>Build better products and services</p> <p>A sentiment analysis system helps companies improve their products and services based on genuine and specific customer feedback. AI technologies identify real-world objects or entities that customers associate with negative sentiment. </p> </li> <li> <p>Analyze at scale</p> <p>Businesses constantly mine information from a vast amount of unstructured data, such as emails, chatbot transcripts, surveys, customer relationship management records, and product feedback. Cloud-based sentiment analysis tools allow businesses to scale the process of uncovering customer emotions in textual data. </p> </li> <li> <p>Real-time results</p> <p>Businesses must be quick to respond to potential crises or market trends in today's fast-changing landscape. Marketers rely on sentiment analysis software to learn what customers feel about the company's brand, products, and services in real time and take immediate actions based on their findings.</p> </li> </ul>"},{"location":"sentiment_analysis/sentiment_analysis/#challenges","title":"\u27a4 Challenges","text":"<ul> <li>Sarcasm and Irony</li> </ul> <p>These linguistic features can completely reverse the sentiment of a statement. Detecting sarcasm and irony is a complex task even for humans, and it's even more challenging for AI systems.</p> <ul> <li>Contextual Understanding</li> </ul> <p>The sentiment of certain words can change based on the context in which they're used. For example, the word \"sick\" can have a negative connotation in a health-related context (\"I'm feeling sick\") but can be positive in a different context (\"That's a sick beat!\").</p> <ul> <li>Negations and Double Negatives</li> </ul> <p>Phrases like \"not bad\" or \"not unimpressive\" can be difficult to interpret correctly because they require understanding of double negatives and other linguistic nuances.</p> <ul> <li>Emojis and Slang</li> </ul> <p>Text data, especially from social media, often contains emojis and slang. The sentiment of these can be hard to determine as their meanings can be subjective and vary across different cultures and communities.</p> <ul> <li>Multilingual Sentiment Analysis</li> </ul> <p>Sentiment analysis becomes significantly more difficult when applied to multiple languages. Direct translation might not carry the same sentiment, and cultural differences can further complicate the analysis.</p> <ul> <li>Aspect-Based Sentiment Analysis</li> </ul> <p>Determining sentiment towards specific aspects within a text can be challenging. For instance, a restaurant review might have a positive sentiment towards the food, but a negative sentiment towards the service.</p>"},{"location":"sentiment_analysis/sentiment_analysis/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>Simple sentiment analysis on sentences with textblob which returns polarity and subjectivity as output.</p> <p>Polarity determines the sentiment of the text. Its values lie in [-1,1] where -1 denotes a highly negative sentiment and 1 denotes a highly positive sentiment.</p> <p>Subjectivity determines whether a text input is factual information or a personal opinion. Its value lies between [0,1] where a value closer to 0 denotes a piece of factual information and a value closer to 1 denotes a personal opinion.</p>"},{"location":"sentiment_analysis/sentiment_analysis/#with-textblob","title":"With Textblob","text":"<pre><code>!pip install textblob\n\nfrom textblob import TextBlob\n\ntext_1 = \"It was really good and delicious\"\ntext_2 = \"I have never seen a worst movie like these\"\n\ndef sentiment(text):\n    pol =  TextBlob(text).sentiment.polarity\n    sub =  TextBlob(text).sentiment.subjectivity\n    return [pol, sub]\n\nresult1 = sentiment(text_1)\nresult2 = sentiment(text_2)\n\nprint(f\"{result1=}\")\nprint(f\"{result2=}\")\n</code></pre> <p>Result</p> <pre><code>result1=[0.85, 0.8]\nresult2=[-1.0, 1.0]\n</code></pre>"},{"location":"sentiment_analysis/sentiment_analysis/#with-nltk","title":"With NLTK","text":"<pre><code>from nltk.sentiment import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\nsia.polarity_scores(\"Wow, NLTK is really powerful!\")\n</code></pre> <p>Result</p> <pre><code>{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}\n</code></pre>"},{"location":"text_classification/intent_detection/","title":"Intent detection","text":"<p>Intent detection is the process of determining the underlying intention or goal behind a given piece of text or spoken language. It involves classifying user inputs into predefined categories, where each category represents a specific intent or purpose.</p>"},{"location":"text_classification/intent_detection/#intent-detection-process","title":"Intent detection process","text":"<p>Intent detection involves several key components, including machine learning algorithms, data preprocessing, training, and evaluation.</p> <pre><code>graph TB\n  A[Data collection &amp; annotation] --&gt; B[Data preprocessing]\n  B --&gt; C[Feature extraction];\n  C --&gt; D[Model training and testing];\n  D --&gt; E[Evaluation]\n  E --&gt; F[Countinuous Monitoring];\n  F --&gt; G[Model update];</code></pre>"},{"location":"text_classification/intent_detection/#data-preprocessing-and-feature-extraction","title":"\u27a4 Data Preprocessing and Feature Extraction","text":"<p>Firstly, in order to prepare input data for model training it needs to clean which is done by data preprocessing. It includes tokenization, remove stop words or other fuzzy data or symbols, lemmatization or stemming followed by feature extraction(converting into numerical form i.e. embeddings).</p>"},{"location":"text_classification/intent_detection/#model-training-and-testing","title":"\u27a4 Model training and testing","text":"<p>Dataset with intent label are trained by splitting it into trainig, testing and validation. In trainig phase model learns to map inputs to their corresponding intent categories by setting model hyperparameters to optimize performance. Naive bias, Support vector machine, Decision trees and random forest, CNN, RNN, BERT and GPT are widely used for intent detection.</p>"},{"location":"text_classification/intent_detection/#evaluation-matrices","title":"\u27a4 Evaluation matrices","text":"<p>After trainig, to check the performance evaluation matrices like F1 score(mean of precision and recall) is used.</p>"},{"location":"text_classification/intent_detection/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>Intent detection with Tfidf and Naivebayes.</p> <p>Imports</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n</code></pre> <p>Create tfidf vector and transform data</p> <pre><code>training_data = [\n     ('i want to buy a jeans pent', 'Buy_a_product'),\n     ('i want to purchase a pair of shoes', 'Buy_a_product'),\n     ('are you selling laptops', 'Buy_a_product'),\n     ('i need an apple jam', 'Buy_a_product'),\n     ('can you please tell me the price of this product', 'Buy_a_product'),\n     ('please give me some discount.', 'negotition'),\n     (\"i cannot afford such price\", 'negotition'),\n     (\"could you negotiate\", \"negotition\"),\n     (\"i agree on your offer\", \"success\"),\n     (\"yes i accepcted your offer\", \"success\"),\n     (\"offer accepted\", \"success\"),\n     (\"agreed\", \"success\"),\n     (\"what is the price of this watch\", \"ask_for_price\"),\n     (\"How much it's cost\", \"ask_for_price\"),\n     (\"i will only give you 3000 for this product\", \"counter_offer\"),\n     (\"Its too costly i can only pay 1500 for it\", \"counter_offer\"),\n]\n\ntest_data = [\n    'your offer',\n    'it will cost you 10000',\n    'i need chocolate',\n    'i want to buy t-shirts',\n    'does it cost 30',\n]\n\nvectorizer = TfidfVectorizer()\ndata = [t[0] for t in training_data]\nintent = [t[1] for t in training_data]\n\nX = vectorizer.fit_transform(data)     # train\nY = vectorizer.transform(test_data)    # test\n</code></pre> <p>Classify data with Naivebayes multinomiaNB model</p> <pre><code>clf = MultinomialNB()\nclf.fit(X, intent)              \nresult = clf.predict(Y) # test\nprob = clf.predict_proba(Y)\nacc = clf.score(X, intent) # train\n\nprint(\"Prediction=\", result)\nprint(\"accuracy=\", acc)\n</code></pre> <p>Result</p> <pre><code>prediction=array(['success', 'Buy_a_product', 'Buy_a_product', 'Buy_a_product',\n       'Buy_a_product'], dtype='&lt;U13')\naccuracy=0.9375\n</code></pre>"},{"location":"text_classification/ner/","title":"Named Entity Recognition","text":"<p>Named Entity Recognition (NER) is a technique in nlp that focuses on identifying and classifying entities from text. NER is the component of information extraction that aims to identify and categorize named entities within unstructured text. NER involves the identification of key information in the text and classification into a set of predefined categories.</p> <p>An entity is the thing that is consistently talked about or refer to in the text, such as person names, organizations, locations, time expressions, quantities, percentages and more predefined categories.</p> <pre><code>Text: \"Argentine captain Lionel Messi won Golden Ball at FIFA world cup 2022\"\n\n- Argentine     LOCATION\n- Lionel Messi  PERSON\n- Golden Ball   OTHER\n- 2022          YEAR\n</code></pre> <p>NER enabling machines to understand and categorize entities in a meaningful manner for various applications like question answering, information retrival, knowledge graph construction, machine translation and text summarization. NER plays important role in part-of-speech (POS) tagging and parsing.</p>"},{"location":"text_classification/ner/#ner-methods","title":"NER methods","text":""},{"location":"text_classification/ner/#lexicon-based-method","title":"\u27a4 Lexicon Based Method","text":"<p>In lexical based approach uses a dictionary with a list of words or terms. The process involves checking if any of these words are present in a given text. However, this approach isn't commonly used because it requires constant updating and careful maintenance of the dictionary to stay accurate and effective.</p>"},{"location":"text_classification/ner/#rule-based-method","title":"\u27a4 Rule Based Method","text":"<p>The Rule Based NER method uses a set of predefined rules guides the extraction of information. These rules are based on patterns and context. Pattern-based rules focus on the structure and form of words, looking at their morphological patterns. On the other hand, context-based rules consider the surrounding words or the context in which a word appears within the text document. This combination of pattern-based and context-based rules enhances the precision of information extraction in NER.</p>"},{"location":"text_classification/ner/#machine-learning-based-method","title":"\u27a4 Machine Learning-Based Method","text":"<p>This approach includes to train the model for multi-class classification using different machine learning algorithms that requires a lot of labelling. Other way is to Conditional random field (CRF) that is implemented by both NLP Speech Tagger and NLTK. It is a probabilistic model that can be used to model sequential data such as words.</p>"},{"location":"text_classification/ner/#deep-learning-based-method","title":"\u27a4 Deep Learning Based Method","text":"<p>Deep learning NER system is much more accurate than other methods as it is capable to assemble words cause it used word embedding, that is capable of understanding the semantic and syntactic relationship between various words. With this approach it is also able to learn analyzes topic specific as well as high level words automatically.</p>"},{"location":"text_classification/ner/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":"<p>With Spacy</p> <pre><code>from spacy import load\n\ntext = \"Tim Cook is the CEO of Apple company founded by Steve Jobs and Steve Wozniak in April 1976 at California.\"\n\nnlp = load(\"en_core_web_sm\")\ndoc = nlp(text)\nprint([(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents])\n</code></pre> <p>Result</p> <pre><code>[('Tim Cook', 0, 8, 'PERSON'), ('Apple', 23, 28, 'ORG'), ('Steve Jobs', 48, 58, 'PERSON'), ('Steve Wozniak', 63, 76, 'PERSON'), ('April 1976', 80, 90, 'DATE'), ('California', 94, 104, 'GPE')]\n</code></pre> <p>With bert</p> <pre><code>from transformers import AutoTokenizer, AutoModelForTokenClassification\nfrom transformers import pipeline\n\ntokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n\nnlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\nresult = nlp(text)\n\nprint(result)\n</code></pre> <p>Result</p> <pre><code>[{'entity': 'B-PER', 'score': 0.9997967, 'index': 1, 'word': 'Tim', 'start': 0, 'end': 3}, {'entity': 'I-PER', 'score': 0.99976486, 'index': 2, 'word': 'Cook', 'start': 4, 'end': 8}, {'entity': 'B-ORG', 'score': 0.9981711, 'index': 7, 'word': 'Apple', 'start': 23, 'end': 28}, {'entity': 'B-PER', 'score': 0.9997534, 'index': 11, 'word': 'Steve', 'start': 48, 'end': 53}, {'entity': 'I-PER', 'score': 0.9996791, 'index': 12, 'word': 'Job', 'start': 54, 'end': 57}, {'entity': 'I-PER', 'score': 0.98890764, 'index': 13, 'word': '##s', 'start': 57, 'end': 58}, {'entity': 'B-PER', 'score': 0.99980503, 'index': 15, 'word': 'Steve', 'start': 63, 'end': 68}, {'entity': 'I-PER', 'score': 0.99975544, 'index': 16, 'word': 'W', 'start': 69, 'end': 70}, {'entity': 'I-PER', 'score': 0.99936086, 'index': 17, 'word': '##oz', 'start': 70, 'end': 72}, {'entity': 'I-PER', 'score': 0.9995459, 'index': 18, 'word': '##nia', 'start': 72, 'end': 75}, {'entity': 'I-PER', 'score': 0.98740005, 'index': 19, 'word': '##k', 'start': 75, 'end': 76}, {'entity': 'B-LOC', 'score': 0.9994041, 'index': 24, 'word': 'California', 'start': 94, 'end': 104}]\n</code></pre>"},{"location":"text_clustering/text_clustering/","title":"Text Clustering","text":"<p>Text Clustering is type of unsupervised learning and the fundamental concept in natural language processing (nlp). The primary goal of text clustering is to organize a collection of documents into groups or clusters, based on the similarity of their content. Text clustering also helps to identify patterns and structures within the data, providing valuable insights into the relationships between documents.</p> <p>Text clustering use vector space models, where documents are represented as numerical vectors in a high-dimensional space. Clustering algorithms then operate on these vector representations to partition the documents into clusters. After, each document is assigned to a cluster based on its similarity to other documents in that cluster. This allows for the exploration and analysis of the resulting clusters to gain insights into the underlying structure of the text data. </p> <p>Text clustering can be used for information retrieval, text summarization and topic modeling, to aid in tasks such as document organization, recommendation systems, and content analysis.</p>"},{"location":"text_clustering/text_clustering/#types-of-clustering","title":"\u27a4 Types of Clustering","text":""},{"location":"text_clustering/text_clustering/#1-k-means-clustering","title":"1. K-means Clustering","text":"<p>K-means clustering is mostly used unsupervised learning clustering algorithm that organizes data points into groups based on similarities. The algorithm operates by iteratively assigning each data point to its nearest cluster centroid and then recalculating the centroids based on the newly formed clusters.</p>"},{"location":"text_clustering/text_clustering/#2-hierarchical-clustering","title":"2. Hierarchical Clustering","text":"<p>Hierarchical clustering is a type of clustering algorithm that groups similar documents into a tree-like structure. It combines the clusters until they encompass all the documents into a single cluster. It is a useful technique for exploring the structure of the data. </p>"},{"location":"text_clustering/text_clustering/#3-latent-dirichlet-allocation-lda","title":"3. Latent Dirichlet Allocation (LDA)","text":"<p>Latent Dirichlet Allocation (LDA) models use probabilistic modeling for topic modeling. LDA assumes that each document is a mixture of topics, and concomitantly, each topic is a distribution over words.</p>"},{"location":"text_clustering/text_clustering/#4-dbscan","title":"4. DBSCAN","text":"<p>DBSCAN is a density-based clustering algorithm that groups similar documents based on their density. DBSCAN works by identifying dense regions of data and grouping them.</p>"},{"location":"text_clustering/text_clustering/#5-fuzzy-clustering","title":"5. Fuzzy Clustering","text":"<p>Fuzzy Clustering is an extension of K-Means that allows a data point to belong to multiple clusters, with a degree of membership. The algorithm uses a membership matrix to represent the fuzzy cluster assignment. Fuzzy Clustering assigns each document to multiple clusters with different degrees of membership.</p>"},{"location":"text_clustering/text_clustering/#6-mean-shift-clustering","title":"6. Mean Shift Clustering","text":"<p>Mean Shift Clustering is a non-parametric clustering algorithm that groups similar documents based on their local maxima.</p>"},{"location":"text_clustering/text_clustering/#7-spectral-clustering","title":"7. Spectral Clustering","text":"<p>Spectral Clustering is a type of clustering algorithm that uses the eigenvalues and eigenvectors of the data matrix to group similar documents.</p>"},{"location":"text_clustering/text_clustering/#steps","title":"\u27a4 Steps","text":"<pre><code>graph LR\n  A[Preprocessing] --&gt; B[Feature extraction];\n  B --&gt; C[Similarity measure];\n  C --&gt; D[Clustering algorithm];\n  D --&gt; E[No. of clusters];\n  E --&gt; F[Clusters];\n</code></pre>"},{"location":"text_clustering/text_clustering/#1-preprocessing","title":"1. Preprocessing","text":"<p>The first step in text clustering is to preprocess the text data that includes cleaning and preprocessing of the text data to eliminate unwanted characters, converting all text to lowercase, removing stop words, stemming and lemmatizing words, and transforming the data into a numerical representation, such as a term frequency-inverse document frequency (TF-IDF) matrix</p>"},{"location":"text_clustering/text_clustering/#2-feature-extraction","title":"2. Feature extraction","text":"<p>The next step is to extract features from the text data that can be used to calculate the similarity between documents. This involves converting the text into a set of numerical features that can be used for clustering. There are many different feature extraction techniques available, such as bag-of-words, TF-IDF, and word embeddings.</p>"},{"location":"text_clustering/text_clustering/#3-similarity-measure","title":"3. Similarity measure","text":"<p>After features extraction, a similarity measure is used to determine the similarity between pairs of documents. This can involve calculating the Euclidean distance, Cosine similarity, or Jaccard similarity between documents</p>"},{"location":"text_clustering/text_clustering/#4-clustering-algorithm","title":"4. Clustering algorithm","text":"<p>The next step is to apply a clustering algorithm to group the documents into clusters. Algorithms include k-means, hierarchical clustering, fuzzy logic and DBSCAN. The choice of algorithm will depend on the nature of the data, the desired number of clusters, and the computational resources available</p>"},{"location":"text_clustering/text_clustering/#5-no-of-clusters","title":"5. No. of clusters","text":"<p>The next step is to choose the number of clusters. Which is very important and critical step in text clustering that not only determines the final output but also plays a pivotal role in the process. There are many different methods available for choosing the number of clusters, such as the elbow method, silhouette score and gap statistic.</p>"},{"location":"text_clustering/text_clustering/#6-clusters","title":"6. Clusters","text":"<p>The final step is to perform the clustering that entails employing the selected clustering algorithm to the feature matrix and subsequently categorizing the documents into clusters.</p>"},{"location":"text_clustering/text_clustering/#applications","title":"\u27a4 Applications","text":"<ul> <li>Document Organizing</li> <li>Topic Modeling</li> <li>Sentiment Analysis</li> <li>Recommandation system</li> </ul>"},{"location":"text_clustering/text_clustering/#example","title":"\u27a4 Example","text":"<ul> <li>K means clustering</li> </ul>"},{"location":"text_similarity/text_similarity/","title":"Text Similarity","text":"<p>Text similarity is useful and one of the active research and application topics in Natural Language Processing. It measures how much the meaning or content of two pieces of text are the same. There are many ways to measure text similarity. </p> <p>Wait!</p> <p>First let's make things clear.</p> <p>There are many ways and methodology includes algorithms, embedding techniques, matrices and pre-trained models. </p> <ul> <li>Embeddings: word2vec, TF-IDF</li> <li>Matrices: Cosinse similarity, Jaccard similarity, Euclidean Distance, Levenshtein Distance</li> <li>Lexical similarity: for clustering and keyword matching</li> <li>Semantic simialrity: for knowledge base, string and statical based</li> <li>Model: BERT, RoBERT, ALBERT, FastText</li> </ul> <p>Note</p> <p>Text similarity is the process to calculate how two words/phrases/documents are close to each other. Semantic similarity is about the meaning closeness while lexical similarity is about the closeness of the word set.</p> <p>Example</p> <pre><code>It might not rain today\nIt might not work today\n</code></pre> <p>It seems very similar according to the lexical similarity, those two phrases are very close and almost identical because they have the same word set('It', 'might', 'not', 'today'). But semantically, they are completely different because they have different meanings ('rain', 'work') despite the similarity of the word set.  </p>"},{"location":"text_similarity/text_similarity/#matrices","title":"\u27a4 Matrices","text":""},{"location":"text_similarity/text_similarity/#1-cosinse-similarity","title":"1. Cosinse similarity","text":"<p>Cosine simiarity is widely used to find similarity between two texts based on the angle between their word vectors. It measures the similarity between two non-zero vectors of an inner product space in general. It is often used to measure the similarity between two documents represented as vectors of word frequencies. </p> <p>To find the similarity of documents, a vector representation of each document is constructed where each dimension of the vector corresponds to a word in the document, and the value of the dimension represents the frequency of that word in the document. Followed by then normalization process to have a unit length. </p> \\[ similarity(A, B) = cos (\\emptyset )= \\frac{A\\bullet B}{\\parallel A \\parallel \\times \\parallel B\\parallel } \\] <p>The cosine similarity is calculated as the dot product of the two vectors divided by the product of their lengths. It measures the cosine of the angle between two embeddings and determines whether they are pointing in the same direction or not. </p> <ul> <li>When the embeddings are pointing in the same direction the angle between them is zero so their cosine similarity is 1 indicates identical documents. </li> <li>When the embeddings are perpendicular to each other the angle between them is 90 degrees and the cosine similarity is 0 indicates no similarity.</li> <li>When the angle between them is 180 degrees the cosine similarity is -1 indicates completely dissimilar.</li> </ul> <p>Cosine similarity is widely used in natural language processing and information retrieval, particularly in document clustering, classification, and recommendation systems.</p> <p>Example</p> <pre><code>import numpy as np \n\nA = np.array([5, 3, 4])\nB = np.array([4, 2, 4])\n\ndot_product = np.dot(A, B)\nmagnitude_A = np.linalg.norm(A)\nmagnitude_B = np.linalg.norm(B)\n\ncosine_similarity = dot_product / (magnitude_A * magnitude_B)\nprint(f\"{cosine_similarity=}\")\n</code></pre>"},{"location":"text_similarity/text_similarity/#2-jaccard-similarity","title":"2. Jaccard similarity","text":"<p>The Jaccard index or the Jaccard similarity, measures the similarity between two sets. It is the intersection of two sentences/texts between which the similarity is being calculated divided by the union of those two which refers to the number of common words over a total number of words. In other words, it is the proportion of common elements between two sets.</p> \\[ J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} \\] <p>The Jaccard Similarity score ranges from 0 to 1. - 1 represents most similar  - 0 represents least similar.</p> <p>The Jaccard index is particularly useful when the presence or absence of elements in the sets is more important than their frequency or order. For example, it can be used to compare the similarity of two documents by considering the sets of words that appear in each document.</p> <p>It is widely used in applications such as data mining, information retrieval and pattern recognition. It is very useful when dealing with sparse or high-dimensional data, where the presence or absence of features is more important than their actual values.</p> <p>Example</p> <pre><code>text1 = {\"It\", \"might\", \"not\", \"rain\", \"today\" }\ntext2 = {\"It\", \"might\", \"not\", \"work\", \"today\"}\nintersection = len(text1.intersection(text2))\nunion = len(text1.union(text2))\n\njaccard_similarity = intersection / union\nprint(f\"{jaccard_similarity=}\")\n</code></pre>"},{"location":"text_similarity/text_similarity/#3-euclidean-distance","title":"3. Euclidean Distance","text":"<p>The Euclidean Distance formula is the most common formula to calculate distance between two points/coordinates in euclidean space. It is calculated as the square root of the sum of the squares of the differences between the corresponding coordinates of the two points.</p> \\[ distance = \\sqrt{\\sum_{i=1}^{n}({x_i}-{y_i})^2} \\] <p>The Euclidean distance ranges from 0 to infinity. - 0 indicates identical vectors  - larger values indicate greater dissimilarity between the vectors.</p> <p>In the context of document similarity, the Euclidean distance can be used to compare the frequency of words in two documents represented as vectors of word frequencies. The Euclidean distance can be extended to spaces of any dimension. It is commonly used in machine learning and data analysis to measure the similarity between two vectors in a high-dimensional space.</p> <p>Euclidean distance is widely used in various applications such as clustering, classification, and anomaly detection. It is particularly useful when dealing with continuous variables or data that can be represented as vectors in a high-dimensional space.</p> <p>Euclidean Distance takes a bit more time and computation power than other two.</p> <p>Example</p> <pre><code>point1 = np.array((1, 2, 3))\npoint2 = np.array((1, 1, 1))\n\nsum_sq = np.sum(np.square(point1 - point2))\neuclidean_distance = np.sqrt(sum_sq)\nprint(f\"{euclidean_distance=}\")\n</code></pre>"},{"location":"text_similarity/text_similarity/#4-levenshtein-distance","title":"4. Levenshtein distance","text":"<p>Levenshtein distance or also called Edit distance, measures the difference between two strings. It is the minimum number of single character insertions, deletions, or substitutions required to transform one string into another.</p> <p>For example, the Levenshtein distance between \u201ckitten\u201d and \u201csitting\u201d is 3, since three single character edits are required to transform \u201ckitten\u201d into \u201csitting\u201d: substitute \u201cs\u201d for \u201ck\u201d, substitute \u201ci\u201d for \u201ce\u201d, and insert \u201cg\u201d at the end.</p> <p>Levenshtein distance is used in various applications such as spell correction, string matching, and DNA analysis.</p> <p>Example</p> <pre><code>A = \"cherry\"\nB = \"berry\"\ndef cal_levenshtein_distance(A, B):\n    N, M = len(A), len(B)\n    # Create an array of size NxM\n    dp = [[0 for i in range(M + 1)] for j in range(N + 1)]\n\n    # Base Case: When N = 0\n    for j in range(M + 1):\n        dp[0][j] = j\n    # Base Case: When M = 0\n    for i in range(N + 1):\n        dp[i][0] = i\n    # Transitions\n    for i in range(1, N + 1):\n        for j in range(1, M + 1):\n            if A[i - 1] == B[j - 1]:\n                dp[i][j] = dp[i-1][j-1]\n            else:\n                dp[i][j] = 1 + min(\n                    dp[i-1][j], # Insertion\n                    dp[i][j-1], # Deletion\n                    dp[i-1][j-1] # Replacement\n                )\n    return dp[N][M]\n</code></pre>"},{"location":"text_similarity/text_similarity/#embeddings","title":"\u27a4 Embeddings","text":""},{"location":"text_similarity/text_similarity/#1-word2vec","title":"1. word2vec","text":"<p>Word2Vec represents the words as high-dimensional vectors so that we get semantically similar words close to each other in the vector space. There are two main architectures for Word2Vec.</p>"},{"location":"text_similarity/text_similarity/#continuous-bag-of-words","title":"Continuous Bag of Words","text":"<p>The CBOW model is a supervised learning neural network model that predicts the center word from the corpus words. It takes one hot encoding of the words as input and the output is the main word that can possibly add some sense to the neighboring words. The objective is to predict the target word based on the context of surrounding words.</p> Input (Context) Output (Target) (I, to) like (like, drink) to (to, coffee) drink (drink, the) coffee (coffee, whole) the (the, day) whole (whole) day"},{"location":"text_similarity/text_similarity/#skip-gram","title":"Skip-gram","text":"<p>The Skip-Gram model works in just the opposite way of the CBOW model. It takes the target word as input and predicts the neighbouring words.</p> Input Output like (I, to) to (like, drink) drink (to, coffee) coffee (drink, the) the (coffee, whole) whole (the, day) day (whole)"},{"location":"text_similarity/text_similarity/#2-tf-idf","title":"2. TF-IDF","text":"<p>TF-IDF is essentially a number that tells you how unique a word or term is across multiple pieces of text. Those numbers are then combined to determine how unique each bit of text is from each other.</p> <ul> <li>Term frequency</li> </ul> <p>The number of times a term occurs in a document is called its term frequency.  It measures how often a word/term appears in a bit of text (document). This is computed as the ratio between the number of times the word/term is in the document and the number of words in the document. The weight of a term that occurs in a document is proportional to the term frequency.</p> <p>Say there is a query \"the red car\" to rank among documents. So in order to find the relevant documents we use the combination ['the', 'red', 'car'] that fetches all the documents having these combination but still there are too many documents with this match. Therefore we count the number of times each term occurs in each document (called term frequency).</p> <ul> <li>Inverse document frequency </li> </ul> <p>There might be the problem with term frequency for a query like \"at the station the\" because the term \"the\" is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word \"the\" more frequently, without giving enough weight to the more meaningful terms \"station\". The term \"the\" is very common so it is not a good keyword to distinguish relevant and non-relevant documents and terms, unlike the least common words \"station\". Hence an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.</p> <p>The specificity of a term can be quantified as an inverse function of the number of documents in which it occurs.</p> <p>The formula for inverse document frequency is a bit more complicated and many software implementations use their own tweaks. That being said, IDF ratio is just the ratio between the number of documents in your corpus and the number of documents with the word you\u2019re evaluating.</p> <p>The TF and IDF parts are multiplied together to get the actual TF-IDF value. This gives us a metric for how much each word makes a document in the corpus unique.</p>"},{"location":"text_similarity/text_similarity/#lexical-similarity","title":"\u27a4 Lexical similarity","text":"<p>Lexical similarity is the similarity between words or set of words in both senteces or text.</p> <p>the cat ate the mouse \\ the mouse ate the cat</p> <p>Both sounds similar according to the word sets ['the', 'cat', 'ate', 'mouse']. The similarity score will be very high as the words in both the sentences are almost same.</p> <p>It is used for Clustering to group similar texts together based on their similarity and Keywords matching for selecting texts based on given keywords like finding resumes with similar skills set keywords.</p>"},{"location":"text_similarity/text_similarity/#semantic-simialrity","title":"\u27a4 Semantic simialrity","text":"<p>Semantic Similarity refers to the degree of similarity between the words. The focus is on the structure and lexical resemblance of words and phrases. Semantic similarity delves into the understanding and meaning of the content. It measures how close or how different the two pieces of word or text are in terms of their meaning and context.</p> <p>Types of Semantic similarity:</p> <ul> <li>Knowledge based: similarity between the concept of corpus.</li> <li>Statistical based: similarity based on learning features vectors from the corpus. String based: combines the above two approaches to find the similarity between non-zero vectors.</li> </ul> <p>Semantic similarity is often used in nlp for question answer, recommandation system, paraphrase identification.</p>"},{"location":"text_similarity/text_similarity/#implementation","title":"\ud83d\udc69\ud83c\udffb\u200d\ud83d\udcbb Implementation","text":""},{"location":"text_similarity/text_similarity/#with-nltk","title":"with nltk","text":"<pre><code>from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef text_similarity(text1, text2):\n    # Tokenize and lemmatize the texts\n    tokens1 = word_tokenize(text1)\n    tokens2 = word_tokenize(text2)\n    lemmatizer = WordNetLemmatizer()\n    tokens1 = [lemmatizer.lemmatize(token) for token in tokens1]\n    tokens2 = [lemmatizer.lemmatize(token) for token in tokens2]\n\n    # Remove stopwords\n    stop_words = stopwords.words('english')\n    tokens1 = [token for token in tokens1 if token not in stop_words]\n    tokens2 = [token for token in tokens2 if token not in stop_words]\n\n    # Create the TF-IDF vectors\n    vectorizer = TfidfVectorizer()\n    vector1 = vectorizer.fit_transform(tokens1)\n    vector2 = vectorizer.transform(tokens2)\n\n    # Calculate the cosine similarity\n    similarity = cosine_similarity(vector1, vector2)\n\n    return similarity\n</code></pre>"},{"location":"text_similarity/text_similarity/#with-sklearn","title":"with sklearn","text":"<pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef text_similarity(text1, text2):\n    # Convert the texts into TF-IDF vectors\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform([text1, text2])\n\n    # Calculate the cosine similarity\n    similarity = cosine_similarity(vectors)\n    return similarity\n</code></pre>"},{"location":"text_similarity/text_similarity/#with-bert","title":"with bert","text":"<pre><code>from transformers import BertTokenizer, BertModel\nfrom torch import tensor, no_grad\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef text_similarity(text1, text2):\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n    model = BertModel.from_pretrained('bert-base-uncased')\n\n    # Tokenize the sentences\n    tokens1 = tokenizer.tokenize(text1)\n    tokens2 = tokenizer.tokenize(text2)\n\n    # Add [CLS] and [SEP] tokens for separating two texts\n    tokens = ['[CLS]'] + tokens1 + ['[SEP]'] + tokens2 + ['[SEP]']\n    # print(f\"{tokens=}\")\n\n    # Convert tokens to input IDs\n    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n    # print(f\"{input_ids=}\")\n\n    # Convert tokens to input IDs\n    input_ids1 = tensor(tokenizer.convert_tokens_to_ids(tokens1)).unsqueeze(0)  # batchsize 1\n    input_ids2 = tensor(tokenizer.convert_tokens_to_ids(tokens2)).unsqueeze(0)  # batchsize 1\n\n    # embeddings\n    with no_grad():\n        outputs1 = model(input_ids1)\n        outputs2 = model(input_ids2)\n        embeddings1 = outputs1.last_hidden_state[:, 0, :]  # [CLS] token\n        embeddings2 = outputs2.last_hidden_state[:, 0, :]  # [CLS] token\n\n    # Calculate similarity\n    similarity_score = cosine_similarity(embeddings1, embeddings2)\n    return similarity_score\n</code></pre>"},{"location":"text_similarity/text_similarity/#with-torch","title":"with torch","text":"<pre><code>import torch\nfrom torch.nn import CosineSimilarity\n\ndef text_similarity(text1, text2):\n    # Custom encoding\n    encoding_dict1 = {char: i for i, char in enumerate(set(''.join([text1])))}\n    encoding_dict2 = {char: i for i, char in enumerate(set(''.join([text2])))}\n\n    tensor1 = torch.tensor([[encoding_dict1[char] for char in string] for string in [text1]], dtype=float)\n    tensor2 = torch.tensor([[encoding_dict2[char] for char in string] for string in [text2]], dtype=float)\n\n    # Size of tensors must be match\n    if tensor1.shape == tensor2.shape:\n        # Calculate the cosine similarity\n        cosine_similarity = CosineSimilarity(dim=0)\n        similarity = cosine_similarity(tensor1, tensor2)\n    else:\n        raise RuntimeError(f\"{tensor1.shape} and {tensor2.shape} are mismatched\")\n\n    return similarity\n</code></pre>"},{"location":"transformer/self_attention_and_transformer/","title":"Self Attention &amp; Transformer","text":"<p>As we know, Attention is all you need<sup>4</sup> introduced this concept which is building block of Transformer.</p> <p>So let's see in detail.</p>"},{"location":"transformer/self_attention_and_transformer/#toc","title":"TOC","text":"<ul> <li>Transformer Architecture</li> <li>RNN for self attention</li> <li>Self attention architecture<ul> <li>QKV </li> <li>Positional representation</li> <li>Future masking</li> </ul> </li> <li>Transformer Block<ul> <li>Multihead self attention</li> <li>Feedforward layer</li> <li>Layer normalization</li> <li>Residual connection</li> </ul> </li> <li>Logit scaling with Softmax</li> <li>Transformer Encoder</li> <li>Transformer Decoder<ul> <li>Cross attention</li> </ul> </li> </ul>"},{"location":"transformer/self_attention_and_transformer/#transformer-architecture","title":"Transformer Architecture","text":"Transformer Architecture[1]"},{"location":"transformer/self_attention_and_transformer/#rnn-for-self-attention","title":"\u27a4 RNN for self attention","text":"RNN Language Model[3] <p>Recurrent Neural Network is able to conditioning all the previous words in corpus. For time stamp \\(t\\) at hidden layer, there are two inputs \\({x_t}\\) and output of previous layer \\({h}_{t-1}\\) which is multiplied by weight matrix \\({W}^{(hh)}\\) and \\({W}^{(hx)}\\) (as U in fig) to product output feature \\({h}_{t}\\), which are further mutiplies by a weight matrix \\({W}^{S}\\) going through Softmax to predict output \\({y}\\) (next word).</p> <p>To put it all together</p> \\[   \\normalsize {ht = \\sigma(W^{(hh)}h_{t\u22121} + W^{(hx)}x_{[t]}) } \\] \\[   \\normalsize {y = \\text{Softmax}(W^{(S)} h_{t})} \\] <p>But the issue with RNNs is the difficulty with which distant tokens in a sequence can interact with each other. </p>"},{"location":"transformer/self_attention_and_transformer/#self-attention-architecture","title":"\u27a4 Self attention architecture","text":"<p>Attention, is a method for taking a query (what we are looking for), and looking up information in a key-value store by picking the values of the key(what we already have in our corpus) most likely matches the query. By averaging overall values, putting more weight on those which correspond to the keys more like the query.</p> <p>A self-attention layer maps input sequences (x1,...,xn) to output sequences of the same length (a1,...,an). When processing each item in the input, the model has access to all of the inputs up to and including the one under consideration, but no access to information about inputs beyond the current one (No future tokens).</p>"},{"location":"transformer/self_attention_and_transformer/#-qkv","title":"- QKV","text":"Query, Key and Value in self attention [1] <p>A token \\({x}_{i}\\) from sequence \\({x}_{1:n}\\), and query \\({q}_{i}\\) for matrix Q. Then for each token \\(x\\) in sequence \\({[x_1, x_2, ... x_n]}\\), we define both key and value with two weight matrices.</p> \\[ \\text{token} \\longrightarrow  x_i \\\\ \\text{query}  \\longrightarrow q_i \\\\ \\text{key}   \\longrightarrow  k_j \\\\ \\text{value}  \\longrightarrow v_j \\\\ \\] <p>So what we basically do is take our element \\(x_{i}\\) and look in its own sequence (it's like looking at our own room in house we are currently standing) with the help of K, Q, V matrices with Softmax.</p>"},{"location":"transformer/self_attention_and_transformer/#-positional-representation","title":"- Positional representation","text":"<p>In the self-attention operation, there's no built-in notion of order. Ther are certain things to consider is   1. the representation of x is not position dependent; it's just Ew for whatever word w   2. there's no dependence on position in the self-attention operations.</p> <p>Therefore to represent positions in self attention we use vectors that are already position-dependent as inputs. So we add embedded representation of the position of a word (P) to its word embedding. Other way is to change self attention operation itself by adding linear bias but it seems little complex so we will be using positinal encoding here.</p> \\[ \\overrightarrow{x}_{i} = P_{i} + x_{i} \\]"},{"location":"transformer/self_attention_and_transformer/#-future-masking","title":"- Future masking","text":"<p>To stop the current token looking into the future tokens, we used masking by zeroed out(-\u221e) next tokens to eliminate further information.</p> \\[ \\begin{bmatrix} q1\\:k1 &amp; -\\infty &amp; -\\infty &amp; -\\infty &amp; -\\infty\\\\ q1\\:k1 &amp; q2\\:k2 &amp; -\\infty &amp; -\\infty &amp; -\\infty\\\\ q1\\:k1 &amp; q2\\:k2 &amp; q3\\:k3 &amp; -\\infty &amp; -\\infty \\\\ q1\\:k1 &amp; q2\\:k2 &amp; q3\\:k3 &amp; q4\\:k4 &amp; -\\infty \\\\ q1\\:k1 &amp; q2\\:k2 &amp; q3\\:k3 &amp; q4\\:k4 &amp; q5\\:k5 \\\\ \\end{bmatrix} \\]"},{"location":"transformer/self_attention_and_transformer/#transformer-block","title":"\u27a4 Transformer Block","text":"Transformer Block [2] <p>Transformer block includes Feedforward layer, Normalization layer, and Residual connection with Mulrihead attention.</p> <p>Computation inside transformer block</p> Steps Description XS = X + selfattention(X) Input X of shape [N, d] XL = LayerNorm(XS) Apply layer normalization XF = FFN(XL) Apply feedforward nn XA = XF + XL Concate outputs HO = LayerNorm(XA) Apply layer normalization to get final Head output"},{"location":"transformer/self_attention_and_transformer/#-multihead-self-attention","title":"- Multihead self attention","text":"Multihead attention [2] <p>In mutlihead attention, each of the multihead self-attention layers is provided with its own set of key, query and value weight matrices. The outputs from each of the layers are concatenated and then projected to d, thus producing an output of the same size as the input so the attention can be followed by layer norm and feedforward and layers can be stacked.</p>"},{"location":"transformer/self_attention_and_transformer/#-feedforward-layer","title":"- Feedforward layer","text":"<p>It's common to apply feed-forward network independently to each word representation after self attention. The feedforward layer contains N position-wise networks, one at each position. Each is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices. The weights are the same for each position, but the parameters are different from layer to layer. Unlike attention, the hidden dimension of the feed-forward network is substantially larger and are independent for each position and so it is efficient to do lot of computation and parameters that can work parallel.</p>"},{"location":"transformer/self_attention_and_transformer/#-layer-normalization","title":"- Layer normalization","text":"<p>Layer normalization (layer norm) is one of many forms of normalization that can be used to reduce uninformative variation in the activations at a layer, providing a more stable input to the next layer. Layer norm is a variation of the standard score, or z-score, from statistics applied to a single vector in a hidden layer. The input to layer norm is a single vector, for a particular token position i, and the output is that vector normalized. Thus layer norm takes as input a single vector of dimensionality d and produces as output a single vector of dimensionality d. </p> <p>To compute layer norm:</p> <ol> <li>computes statistics across the activations at a layer to estimate the mean and variance of the activations</li> <li>normalizes the activations with respect to those estimates, while optionally learning (as parameters) an elementwise additive bias and multiplicative gain by which to sort of de-normalize the activations in a predictable way.</li> </ol>"},{"location":"transformer/self_attention_and_transformer/#-residual-connection","title":"- Residual connection","text":"<p>Residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate layer. Allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct access to information from lower layer. Residual connections can be implemented by simply adding a layer's input vector to its output vector before passing it forward.</p> \\[ \\text{fresidual}\\:(h_{1:n}) = f(h_{1:n}) + h_{1:n} \\]"},{"location":"transformer/self_attention_and_transformer/#logit-scaling-with-softmax","title":"\u27a4 Logit scaling with Softmax","text":"<p>The dot product part comes from the fact that we're computing dot products \\(q_{i}^{T}\\:k_{j}\\). The intuition of scaling is that, when the dimensionality d of the vectors are dotting grows large, the dot product of even random vectors grows roughly as \\(\\sqrt{d}\\). So, we normalize the dot products by \\(\\sqrt{d}\\) to stop scaling.</p> \\[ \\normalsize{ \\text{softmax} = \\left(\\frac{x_{1:n}QK^{T}x_{1:n}^{T}}{\\sqrt{d}}\\right) } \\]"},{"location":"transformer/self_attention_and_transformer/#transformer-encoder","title":"\u27a4 Transformer Encoder","text":"Transformer Encoder [1] <p>A Transformer Encoder takes a single sequence \\(w_{1:n}\\), and performs no future masking at this stage, so even the  first token can see the whole future of the sequence when building its representation. It embeds the sequence with E to make \\(x_{1:n}\\) input format, adds the position representation, and then applies a stack of independently parameterized Encoder Blocks, each of which consisting of multihead attention with Add &amp; Norm, and feed-forward with Add &amp; Norm. So, the output of each Block is the input to the next. </p>"},{"location":"transformer/self_attention_and_transformer/#transformer-decoder","title":"\u27a4 Transformer Decoder","text":"Transformer Decoder [1] <p>Now to solve the problem with encoder (as we try to build autoregressive model), we use transformer decoder with future masking at each self attention as we seen above.</p>"},{"location":"transformer/self_attention_and_transformer/#-cross-attention","title":"- Cross attention","text":"<p>As in name suggests, unlike self attention(recall looking in same room within house) which looks within own sequence, cross attetion uses one sequence to define the keys and values of self-attention from encoder, and another sequence (from other room) to define the queries (generate intermediate representation of output sequence).</p>"},{"location":"transformer/self_attention_and_transformer/#example","title":"Example","text":"<ul> <li>Understanding Attention Mechanism</li> <li>Understanding Transformer architecture</li> </ul>"},{"location":"transformer/self_attention_and_transformer/#references","title":"References","text":"<ol> <li> <p>https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/readings/cs224n-self-attention-transformers-2023_draft.pdf\u00a0\u21a9</p> </li> <li> <p>https://web.stanford.edu/~jurafsky/slpdraft/10.pdf\u00a0\u21a9</p> </li> <li> <p>https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1244/readings/cs224n-2019-notes05-LM_RNN.pdf\u00a0\u21a9</p> </li> <li> <p>https://arxiv.org/abs/1706.03762\u00a0\u21a9</p> </li> </ol>"}]}