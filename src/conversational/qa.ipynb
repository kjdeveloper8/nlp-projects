{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb1fdeb-93ad-44c6-bfd4-8112b47cc338",
   "metadata": {},
   "source": [
    "# Question Answer in NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e872e-fb43-41eb-afc5-d7afe5c87409",
   "metadata": {},
   "source": [
    "Implementing Question Answering model with the BertForQuestionAnswering model fine tuned on Stanford Question Answering Dataset (SQuAD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8da900f-38ac-4b2d-a4ce-7fb72ab61404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krinaljoshi/.local/share/virtualenvs/nlp-projects-KSis0_H_/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155819b3-392d-4f48-bc38-3e882d42000c",
   "metadata": {},
   "source": [
    "Load the model for QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c22909ba-1596-4727-9a31-e73468173efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf08757e-dce1-4b74-b550-4eb3977f8ce4",
   "metadata": {},
   "source": [
    "Sample QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0e9fb3-8306-4bc6-bf7a-1ea7c3c39286",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where is the Great Barrier Reef located?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3887d73-4e08-4d46-98cd-18619d53e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_text = \"The Great Barrier Reef is located in the Coral Sea, off the coast of Australia. It is the largest coral reef system in the world, stretching over 2,300 km and covering an area of approximately 344,400 kmÂ². The Great Barrier Reef is home to a diverse range of marine life and is considered one of the seven natural wonders of the world. It is also a UNESCO World Heritage Site threatened by climate change and other environmental factors.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59098a7-fd20-4859-957f-4c4c0f355e8b",
   "metadata": {},
   "source": [
    "Tokenize input id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e4a0cc7-c058-44c7-b96f-b00069259ad4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 2073,\n",
       " 2003,\n",
       " 1996,\n",
       " 2307,\n",
       " 8803,\n",
       " 12664,\n",
       " 2284,\n",
       " 1029,\n",
       " 102,\n",
       " 1996,\n",
       " 2307,\n",
       " 8803,\n",
       " 12664,\n",
       " 2003,\n",
       " 2284,\n",
       " 1999,\n",
       " 1996,\n",
       " 11034,\n",
       " 2712,\n",
       " 1010,\n",
       " 2125,\n",
       " 1996,\n",
       " 3023,\n",
       " 1997,\n",
       " 2660,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 1996,\n",
       " 2922,\n",
       " 11034,\n",
       " 12664,\n",
       " 2291,\n",
       " 1999,\n",
       " 1996,\n",
       " 2088,\n",
       " 1010,\n",
       " 10917,\n",
       " 2058,\n",
       " 1016,\n",
       " 1010,\n",
       " 3998,\n",
       " 2463,\n",
       " 1998,\n",
       " 5266,\n",
       " 2019,\n",
       " 2181,\n",
       " 1997,\n",
       " 3155,\n",
       " 29386,\n",
       " 1010,\n",
       " 4278,\n",
       " 3186,\n",
       " 1012,\n",
       " 1996,\n",
       " 2307,\n",
       " 8803,\n",
       " 12664,\n",
       " 2003,\n",
       " 2188,\n",
       " 2000,\n",
       " 1037,\n",
       " 7578,\n",
       " 2846,\n",
       " 1997,\n",
       " 3884,\n",
       " 2166,\n",
       " 1998,\n",
       " 2003,\n",
       " 2641,\n",
       " 2028,\n",
       " 1997,\n",
       " 1996,\n",
       " 2698,\n",
       " 3019,\n",
       " 16278,\n",
       " 1997,\n",
       " 1996,\n",
       " 2088,\n",
       " 1012,\n",
       " 2009,\n",
       " 2003,\n",
       " 2036,\n",
       " 1037,\n",
       " 12239,\n",
       " 2088,\n",
       " 4348,\n",
       " 2609,\n",
       " 5561,\n",
       " 2011,\n",
       " 4785,\n",
       " 2689,\n",
       " 1998,\n",
       " 2060,\n",
       " 4483,\n",
       " 5876,\n",
       " 1012,\n",
       " 102]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, answer_text)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ad56436-0f54-49bd-9da4-09b4712e24e7",
   "metadata": {},
   "source": [
    "Create attention mask (sequence of 1s and 0s) indicating which tokens in the input_ids sequence should be attended to by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0080c872-f99a-4bc3-a057-61bcb2ea7b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = [1] * len(input_ids)\n",
    "len(attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e07e01-f178-4903-b95c-81bed95c3838",
   "metadata": {},
   "source": [
    "get the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ddef65-b9d0-4b2d-914a-e29b29d24ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.tensor([input_ids]), attention_mask=torch.tensor([attention_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64128f2-dd81-4d00-82bc-5f22654b9b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-6.4106, -5.9257, -7.6330, -7.7274, -8.1191, -8.6230, -8.6095, -7.8516,\n",
       "         -8.8621, -6.4106,  1.8369, -0.6083, -3.4098, -4.1775, -2.3809,  0.0152,\n",
       "          4.8730,  6.7679,  7.2950, -1.0516, -3.6109,  1.6546, -2.7443, -2.8114,\n",
       "         -4.9304,  1.8826, -6.4102, -3.4538, -7.1368, -6.2586, -5.6499, -5.0297,\n",
       "         -6.4085, -6.6760, -7.3548, -7.1828, -5.7484, -7.8486, -4.7000, -7.1409,\n",
       "         -5.4418, -8.2328, -7.1212, -6.8992, -8.1358, -5.6112, -7.2474, -6.3876,\n",
       "         -8.2483, -5.2105, -4.3644, -8.1883, -7.1706, -6.6519, -8.0507, -4.0760,\n",
       "         -4.1507, -7.0613, -7.5427, -8.0758, -6.9090, -8.7749, -8.1034, -7.0426,\n",
       "         -8.2258, -8.8082, -6.8397, -7.7390, -8.6729, -7.8738, -7.2368, -7.4564,\n",
       "         -8.5908, -8.2528, -6.6169, -6.9555, -7.2141, -8.4457, -7.1364, -6.9217,\n",
       "         -8.2917, -6.2226, -8.0376, -8.0975, -6.7058, -4.2892, -6.3594, -7.3548,\n",
       "         -7.0044, -6.3021, -8.6310, -7.1910, -8.1402, -8.9861, -8.5225, -7.4956,\n",
       "         -7.9532, -8.7478, -6.4106]], grad_fn=<CloneBackward0>), end_logits=tensor([[-2.0405, -6.0909, -6.6392, -7.6310, -7.6124, -6.7632, -6.5812, -7.1516,\n",
       "         -6.6786, -2.0405, -5.5186, -6.0083, -4.7783, -3.7040, -5.7127, -3.6903,\n",
       "         -2.3458, -1.7938, -0.0579,  7.2070,  2.9097, -2.8488, -4.5140, -0.5957,\n",
       "         -1.9185,  5.6021, -2.0400, -5.2573, -6.2580, -7.2043, -5.8797, -6.1305,\n",
       "         -3.2299, -3.8248, -7.1364, -7.2384, -2.5338, -3.5152, -6.2300, -6.8678,\n",
       "         -5.9512, -6.3770, -5.0731, -3.3535, -6.2062, -6.9472, -6.6389, -4.8858,\n",
       "         -6.9160, -6.8074, -5.2258, -6.4833, -4.7468, -2.4184, -2.7813, -7.6225,\n",
       "         -7.7727, -6.1044, -3.6202, -7.0726, -7.1820, -7.5183, -7.5921, -6.8011,\n",
       "         -6.7319, -7.5002, -6.5113, -4.7903, -6.6027, -7.3981, -7.2246, -7.0312,\n",
       "         -7.3031, -7.3878, -6.5330, -6.6063, -5.8751, -7.6981, -6.6880, -3.9501,\n",
       "         -4.5712, -6.8734, -7.2504, -7.3283, -7.6111, -4.8176, -5.9847, -5.2425,\n",
       "         -4.1854, -6.2733, -7.5071, -6.9217, -5.6609, -7.7718, -7.6322, -6.3099,\n",
       "         -4.9555, -5.5228, -2.0405]], grad_fn=<CloneBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36a6d2-90d3-43af-9c4e-351c2bc2f8c4",
   "metadata": {},
   "source": [
    "get the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c50d66d1-8c9f-474b-bf9a-a757d7dfea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = torch.argmax(output[0][0, :len(input_ids) - input_ids.index(tokenizer.sep_token_id)])\n",
    "end_index = torch.argmax(output[1][0, :len(input_ids) - input_ids.index(tokenizer.sep_token_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88109f35-03eb-4c2c-93ac-08ca39eecb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = tokenizer.decode(input_ids[start_index:end_index + 1], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aa028a3-b6ae-41c1-9318-4c8c16064c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coral sea'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b420ce93-4101-4acc-98cb-d227bbab1fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
